# Test Design: Story 3.2 - Python Pipeline Modifications for Web Interface

**Date:** 2025-10-19
**Designer:** Quinn (Test Architect)
**Story:** 3.2 - Python Pipeline Modifications for Web Interface

---

## Test Strategy Overview

- **Total test scenarios:** 28
- **Unit tests:** 14 (50%)
- **Integration tests:** 11 (39%)
- **E2E tests:** 3 (11%)
- **Priority distribution:** P0: 17, P1: 8, P2: 3

**Key Testing Focus:**
1. Backward compatibility with existing CLI modes
2. JSON parsing robustness with variable LLM outputs
3. Track selection logic correctness
4. PDF extraction reliability
5. Pipeline execution consistency (CLI vs web modes)

**Test Strategy Rationale:**
- Heavy unit testing for parsing logic (LLM outputs are unpredictable)
- Integration tests for file I/O and pipeline orchestration
- Minimal E2E tests focused on critical user paths
- High P0 ratio reflects production stability requirements

---

## Test Scenarios by Acceptance Criteria

### AC1: Add `--input-file` CLI argument

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-UNIT-001 | Unit | P1 | Verify `--input-file` argument accepted by argparse | Pure argument validation |
| 3.2-UNIT-002 | Unit | P1 | Verify `--input-file` with valid PDF path | Input validation logic |
| 3.2-UNIT-003 | Unit | P2 | Verify `--input-file` with invalid path returns clear error | Edge case handling |
| 3.2-INT-001 | Integration | P1 | Verify `--input-file` reads actual PDF from `/tmp/` | File system interaction |

**Coverage:** 4 scenarios (3 unit, 1 integration)

---

### AC2: Add `--run-id` CLI argument

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-UNIT-004 | Unit | P1 | Verify `--run-id` argument accepted by argparse | Pure argument validation |
| 3.2-UNIT-005 | Unit | P1 | Verify `--run-id` with alphanumeric identifier | Input validation logic |
| 3.2-UNIT-006 | Unit | P2 | Verify `--run-id` with special characters handled correctly | Edge case handling |

**Coverage:** 3 scenarios (3 unit)

---

### AC3: Add `--selected-track` CLI argument

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-UNIT-007 | Unit | P0 | Verify `--selected-track` only accepts 1 or 2 | Critical business logic |
| 3.2-UNIT-008 | Unit | P0 | Verify `--selected-track` rejects invalid values (0, 3, "foo") | Input validation boundary |
| 3.2-INT-002 | Integration | P0 | Verify selected_track parameter flows to `save_output()` | Cross-component data flow |

**Coverage:** 3 scenarios (2 unit, 1 integration)

---

### AC4: Create `run_from_uploaded_file()` function

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-INT-003 | Integration | P0 | Verify function executes all 5 stages sequentially | Core workflow orchestration |
| 3.2-INT-004 | Integration | P0 | Verify function returns exit code 0 on success | Error handling contract |
| 3.2-INT-005 | Integration | P1 | Verify function returns non-zero on stage failure | Error handling contract |

**Coverage:** 3 scenarios (3 integration)

---

### AC5: Output directory creation and logging setup

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-UNIT-009 | Unit | P1 | Verify output directory path format: `data/test-outputs/{run_id}/` | Path construction logic |
| 3.2-INT-006 | Integration | P0 | Verify output directory created with correct permissions | File system interaction |
| 3.2-INT-007 | Integration | P0 | Verify log file created at `{run_id}/logs/pipeline.log` | Logging infrastructure |

**Coverage:** 3 scenarios (1 unit, 2 integration)

---

### AC6: PDF reading with PyPDF library

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-INT-008 | Integration | P0 | Verify PDF text extraction from valid single-page PDF | Core PDF processing |
| 3.2-INT-009 | Integration | P0 | Verify PDF text extraction from multi-page PDF | Real-world scenario |
| 3.2-INT-010 | Integration | P1 | Verify graceful error handling for corrupted PDF | Error resilience |
| 3.2-E2E-001 | E2E | P1 | Verify PDF extraction matches CLI mode output quality | Cross-mode consistency |

**Coverage:** 4 scenarios (3 integration, 1 E2E)

---

### AC7: Stage logging messages

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-UNIT-010 | Unit | P2 | Verify log messages format: "Starting Stage {N}: {Name}" | String formatting logic |
| 3.2-INT-011 | Integration | P1 | Verify all 10 log messages appear in pipeline.log (5 starts + 5 completions) | Logging integration |

**Coverage:** 2 scenarios (1 unit, 1 integration)

---

### AC8-10: Stage 1 JSON output with parsing

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-UNIT-011 | Unit | P0 | Verify `_parse_inspirations()` extracts track 1 title and summary | Core parsing logic |
| 3.2-UNIT-012 | Unit | P0 | Verify `_parse_inspirations()` extracts track 2 title and summary | Core parsing logic |
| 3.2-UNIT-013 | Unit | P0 | Verify `_parse_inspirations()` handles extra whitespace | LLM output variation |
| 3.2-UNIT-014 | Unit | P0 | Verify `_parse_inspirations()` handles missing header fallback | Error resilience |
| 3.2-UNIT-015 | Unit | P0 | Verify `_empty_track()` returns valid fallback structure | Fallback mechanism |
| 3.2-UNIT-016 | Unit | P0 | Verify JSON structure matches spec: `{ selected_track, track_1, track_2, completed_at }` | Data contract validation |
| 3.2-UNIT-017 | Unit | P0 | Verify `selected_track` is single integer (1 or 2), not array | Critical data structure |
| 3.2-E2E-002 | E2E | P0 | Verify JSON file created at `stage1/inspirations.json` | End-to-end file creation |

**Coverage:** 8 scenarios (7 unit, 1 E2E)

**Note:** Existing unit tests (`tests/test_stage1_json_output.py`) already cover 5 of these scenarios. Scenarios 3.2-UNIT-016, 3.2-UNIT-017, and 3.2-E2E-002 represent additional coverage needed.

---

### AC11: Update `STAGE1_TEMPLATE` prompt

#### Scenarios

**Coverage:** Manual verification (prompt engineering - not automated)

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| *Manual* | Manual | P1 | Verify prompt explicitly requests "exactly 2 tracks" | Prompt engineering QA |
| *Manual* | Manual | P1 | Verify prompt includes format instructions | Prompt engineering QA |

**Note:** Prompt quality assessed through integration test results (track parsing success rate).

---

### AC12: Process only selected track through Stages 2-5

#### Scenarios

**Coverage:** Covered by AC4 integration tests (pipeline orchestration)

---

### AC13: Backward compatibility with existing CLI modes

#### Scenarios

| ID | Level | Priority | Test | Justification |
|---|---|---|---|---|
| 3.2-E2E-003 | E2E | P0 | Verify `--batch` mode executes without regression | Critical backward compatibility |
| 3.2-INT-012 | Integration | P0 | Verify `--input` + `--brand` mode executes without regression | Critical backward compatibility |

**Coverage:** 2 scenarios (1 integration, 1 E2E)

**Regression Testing Requirements:**
- Compare Stage 5 opportunity card outputs (before/after modification)
- Verify markdown output format unchanged
- Validate execution time remains within 10% tolerance

---

### AC14: Pipeline output quality consistency

#### Scenarios

**Coverage:** Covered by AC13 E2E tests (output comparison)

---

## Risk Coverage Mapping

### RISK-001: JSON Parsing Failures (High Probability × Medium Impact)

**Mitigated by:**
- 3.2-UNIT-011 through 3.2-UNIT-015 (Parsing logic validation)
- 3.2-UNIT-014 (Fallback mechanism)
- 3.2-E2E-002 (End-to-end JSON creation)

**Coverage:** 7 test scenarios

---

### RISK-002: PDF Reading Failures (Medium Probability × High Impact)

**Mitigated by:**
- 3.2-INT-008, 3.2-INT-009 (Valid PDF extraction)
- 3.2-INT-010 (Corrupted PDF handling)
- 3.2-E2E-001 (Cross-mode consistency)

**Coverage:** 4 test scenarios

---

### RISK-003: Backward Compatibility Breakage (Low Probability × Critical Impact)

**Mitigated by:**
- 3.2-E2E-003 (Batch mode regression)
- 3.2-INT-012 (CLI mode regression)

**Coverage:** 2 test scenarios (both P0)

**Additional Safeguards:**
- Manual smoke testing of all 3 CLI modes before deployment
- Version-controlled test fixture outputs for comparison

---

### RISK-004: Track Selection Logic Errors (Medium Probability × High Impact)

**Mitigated by:**
- 3.2-UNIT-007, 3.2-UNIT-008 (Input validation)
- 3.2-INT-002 (Data flow validation)
- 3.2-UNIT-017 (Data structure validation)

**Coverage:** 4 test scenarios

---

## Test Execution Strategy

### Phase 1: Unit Tests (Fail Fast)

Execute in this order:

1. **P0 Parsing Logic** (3.2-UNIT-011 through 3.2-UNIT-017)
   - These tests validate the highest risk area (LLM output parsing)
   - Run first to identify core logic issues immediately

2. **P0 Track Selection** (3.2-UNIT-007, 3.2-UNIT-008)
   - Critical business logic validation

3. **P1 CLI Arguments** (3.2-UNIT-001, 3.2-UNIT-002, 3.2-UNIT-004, 3.2-UNIT-005)
   - Standard input validation

4. **P2 Edge Cases** (3.2-UNIT-003, 3.2-UNIT-006, 3.2-UNIT-010)
   - Run last, can skip if time-constrained

**Expected execution time:** <30 seconds

---

### Phase 2: Integration Tests

Execute in this order:

1. **P0 Core Pipeline** (3.2-INT-003, 3.2-INT-004)
   - Validates end-to-end pipeline execution

2. **P0 Infrastructure** (3.2-INT-006, 3.2-INT-007, 3.2-INT-008, 3.2-INT-009)
   - File system and PDF processing

3. **P0 Backward Compatibility** (3.2-INT-012)
   - Critical regression prevention

4. **P0 Data Flow** (3.2-INT-002)
   - Track selection parameter propagation

5. **P1 Error Handling** (3.2-INT-005, 3.2-INT-010, 3.2-INT-011)
   - Secondary validation scenarios

6. **P1 File Operations** (3.2-INT-001)
   - File system edge cases

**Expected execution time:** 2-5 minutes (depends on LLM API latency)

---

### Phase 3: E2E Tests

Execute in this order:

1. **P0 Batch Mode Regression** (3.2-E2E-003)
   - Most critical backward compatibility test

2. **P0 JSON File Creation** (3.2-E2E-002)
   - Validates complete web execution path

3. **P1 PDF Extraction Consistency** (3.2-E2E-001)
   - Cross-mode quality validation

**Expected execution time:** 5-10 minutes (full pipeline runs)

---

## Test Data Requirements

### PDF Test Fixtures

| Fixture | Purpose | Tests |
|---|---|---|
| `savannah-bananas.pdf` | Single-page, clean text | 3.2-INT-008, 3.2-E2E-001 |
| `multi-page-sample.pdf` | Multi-page document | 3.2-INT-009 |
| `corrupted.pdf` | Malformed PDF | 3.2-INT-010 |
| `empty.pdf` | Zero-content PDF | 3.2-INT-010 |

**Location:** `data/test-inputs/`

---

### LLM Output Fixtures

| Fixture | Purpose | Tests |
|---|---|---|
| `stage1-perfect-format.md` | Ideal LLM output | 3.2-UNIT-011, 3.2-UNIT-012 |
| `stage1-extra-whitespace.md` | LLM output with spacing variations | 3.2-UNIT-013 |
| `stage1-missing-header.md` | Malformed LLM output | 3.2-UNIT-014 |
| `stage1-empty.md` | Empty LLM response | 3.2-UNIT-015 |

**Location:** `tests/fixtures/`

---

### Brand Profile Fixtures

| Fixture | Purpose | Tests |
|---|---|---|
| `lactalis-canada.yaml` | Existing brand profile | All integration tests |

**Location:** `data/brand-profiles/`

---

## Test Coverage Analysis

### Coverage by Level

| Level | Scenarios | Percentage | Rationale |
|---|---|---|---|
| Unit | 14 | 50% | High due to parsing logic complexity |
| Integration | 11 | 39% | File I/O and pipeline orchestration |
| E2E | 3 | 11% | Minimal, focused on critical paths |

**Test Pyramid Compliance:** ✅ Healthy distribution (prefer unit > integration > E2E)

---

### Coverage by Priority

| Priority | Scenarios | Percentage | Rationale |
|---|---|---|---|
| P0 | 17 | 61% | High due to production stability requirements |
| P1 | 8 | 29% | Standard feature validation |
| P2 | 3 | 11% | Edge cases and polish |
| P3 | 0 | 0% | No low-priority scenarios identified |

**Priority Distribution Assessment:** ✅ Appropriate for production-critical feature

---

### Coverage Gaps

**None identified.** All acceptance criteria have test coverage.

**Validation:**
- AC1-AC3: CLI arguments (7 tests)
- AC4: Pipeline orchestration (3 tests)
- AC5: Infrastructure setup (3 tests)
- AC6: PDF processing (4 tests)
- AC7: Logging (2 tests)
- AC8-AC10: JSON parsing (8 tests)
- AC11: Prompt engineering (manual validation)
- AC12: Track filtering (covered by AC4)
- AC13: Backward compatibility (2 tests)
- AC14: Output quality (covered by AC13)

---

## Automated Test Implementation Status

### Existing Tests (Implemented)

**File:** `tests/test_stage1_json_output.py`

1. `test_parse_tracks_valid_output()` → Covers 3.2-UNIT-011, 3.2-UNIT-012
2. `test_parse_tracks_with_whitespace()` → Covers 3.2-UNIT-013
3. `test_parse_tracks_missing_header()` → Covers 3.2-UNIT-014
4. `test_empty_track()` → Covers 3.2-UNIT-015
5. `test_json_structure()` → Covers 3.2-UNIT-016

**Status:** ✅ All passing

---

### Recommended Additional Tests (Not Yet Implemented)

#### High Priority (Implement Before Merge)

1. **3.2-UNIT-017:** Verify `selected_track` is single integer
   - **Implementation:** Add assertion to `test_json_structure()` in existing test file
   - **Estimated effort:** 5 minutes

2. **3.2-INT-002:** Verify selected_track parameter flows to `save_output()`
   - **Implementation:** New integration test with mocked LLM output
   - **Estimated effort:** 15 minutes

3. **3.2-INT-003, 3.2-INT-004:** Verify pipeline executes all 5 stages
   - **Implementation:** New integration test calling `run_from_uploaded_file()`
   - **Estimated effort:** 30 minutes

4. **3.2-E2E-003:** Verify `--batch` mode regression
   - **Implementation:** Shell script comparing before/after outputs
   - **Estimated effort:** 20 minutes

**Total high-priority testing effort:** ~70 minutes

---

#### Medium Priority (Implement Before Release)

5. **3.2-INT-008, 3.2-INT-009, 3.2-INT-010:** PDF extraction tests
   - **Implementation:** New test file `tests/test_pdf_extraction.py`
   - **Estimated effort:** 45 minutes

6. **3.2-E2E-002:** Verify JSON file creation end-to-end
   - **Implementation:** Full pipeline run with file system validation
   - **Estimated effort:** 30 minutes

**Total medium-priority testing effort:** ~75 minutes

---

#### Low Priority (Defer to Post-Release)

7. **3.2-UNIT-001 through 3.2-UNIT-010:** CLI argument and logging tests
   - **Implementation:** New test file `tests/test_cli_arguments.py`
   - **Estimated effort:** 60 minutes

**Total low-priority testing effort:** ~60 minutes

---

## Quality Gates

### Pre-Merge Gate

**Requirements:**
- ✅ All existing unit tests passing (5/5)
- 🟡 High-priority additional tests implemented and passing (0/4)
- 🟡 Code coverage >80% for modified files
- 🟡 Manual smoke test of all 3 CLI modes

**Gate Decision:** CONCERNS (see recommended actions below)

---

### Pre-Release Gate

**Requirements:**
- All pre-merge requirements met
- Medium-priority tests implemented and passing
- Manual QA validation of JSON parsing with 10+ LLM outputs
- Performance regression test (pipeline execution time within 10% of baseline)

**Gate Decision:** Pending implementation completion

---

## Recommended Test Automation Setup

### Test Frameworks

```python
# pytest for all Python tests
# pytest-cov for coverage reporting
# pytest-mock for mocking LLM calls

# Install:
pip install pytest pytest-cov pytest-mock
```

---

### Test Execution Commands

```bash
# Run all tests
pytest tests/

# Run only unit tests
pytest tests/ -m unit

# Run with coverage
pytest tests/ --cov=pipeline --cov-report=html

# Run specific test file
pytest tests/test_stage1_json_output.py -v

# Run P0 tests only
pytest tests/ -m "priority_p0"
```

---

### Test Markers (Add to pytest.ini)

```ini
[pytest]
markers =
    unit: Unit tests (fast, no external dependencies)
    integration: Integration tests (moderate speed, may use test DB/files)
    e2e: End-to-end tests (slow, full pipeline runs)
    priority_p0: Critical tests that must pass
    priority_p1: High-priority tests
    priority_p2: Medium-priority tests
```

---

## Maintenance Recommendations

### Test Stability

**Potential Flaky Tests:**
- 3.2-INT-008, 3.2-INT-009 (PDF extraction may vary by pypdf version)
- All LLM-dependent integration tests (API latency variations)

**Mitigation:**
- Mock LLM calls in integration tests
- Use pytest retry plugin for E2E tests
- Pin pypdf version in requirements.txt

---

### Test Data Management

**Best Practices:**
- Version control all test fixtures in `tests/fixtures/`
- Use deterministic test data (no random generation)
- Document expected outputs alongside inputs
- Refresh LLM output fixtures quarterly (models evolve)

---

### Continuous Monitoring

**Metrics to Track:**
- JSON parsing success rate (should be >95%)
- Test execution time trends (watch for degradation)
- Test failure patterns (identify recurring issues)

---

## Summary

**Test Design Quality:** ✅ Comprehensive

**Risk Coverage:** ✅ All major risks addressed

**Test Pyramid Balance:** ✅ Healthy distribution

**Automation Readiness:** 🟡 Partial (existing tests pass, additional tests recommended)

**Recommendation:** Implement high-priority additional tests (3.2-UNIT-017, 3.2-INT-002, 3.2-INT-003, 3.2-E2E-003) before merging to main branch. Current implementation is functional but lacks sufficient integration-level validation for production deployment.

**Estimated Total Testing Effort:**
- Existing tests: ✅ Complete
- High-priority additions: ~70 minutes
- Medium-priority additions: ~75 minutes
- Low-priority additions: ~60 minutes
- **Total remaining effort:** 3-4 hours for full test suite

---

## Appendix: Test Scenario Details

### Example Test Implementations

#### Unit Test Example (3.2-UNIT-017)

```python
def test_selected_track_is_single_integer():
    """Verify selected_track is integer (1 or 2), not array."""
    from pipeline.stages.stage1_input_processing import Stage1Chain
    from pathlib import Path
    import json

    stage1 = Stage1Chain()
    output_dir = Path("/tmp/test-run-001")
    output_dir.mkdir(parents=True, exist_ok=True)

    sample_output = """
## Track 1: Innovation Title
Summary of track 1 content.

## Track 2: Another Title
Summary of track 2 content.
    """

    # Save output with selected_track=1
    stage1.save_output(sample_output, output_dir, selected_track=1)

    # Read JSON file
    json_path = output_dir / "stage1" / "inspirations.json"
    with open(json_path, 'r') as f:
        data = json.load(f)

    # Assertions
    assert isinstance(data["selected_track"], int), "selected_track must be integer"
    assert data["selected_track"] in [1, 2], "selected_track must be 1 or 2"
    assert data["selected_track"] == 1, "selected_track should match parameter"
```

---

#### Integration Test Example (3.2-INT-003)

```python
def test_run_from_uploaded_file_executes_all_stages(mocker):
    """Verify pipeline executes all 5 stages sequentially."""
    from scripts.run_pipeline import run_from_uploaded_file
    from pathlib import Path

    # Setup
    input_file = Path("data/test-inputs/savannah-bananas.pdf")
    brand_id = "lactalis-canada"
    run_id = "test-run-integration-001"

    # Mock LLM calls to avoid API usage
    mocker.patch('pipeline.stages.stage1_input_processing.Stage1Chain.run')
    mocker.patch('pipeline.stages.stage2_amplification.Stage2Chain.run')
    mocker.patch('pipeline.stages.stage3_translation.Stage3Chain.run')
    mocker.patch('pipeline.stages.stage4_contextualization.Stage4Chain.run')
    mocker.patch('pipeline.stages.stage5_opportunities.Stage5Chain.run')

    # Execute
    exit_code = run_from_uploaded_file(
        str(input_file),
        brand_id,
        run_id,
        selected_track=1
    )

    # Assertions
    assert exit_code == 0, "Pipeline should complete successfully"

    # Verify output directory exists
    output_dir = Path(f"data/test-outputs/{run_id}")
    assert output_dir.exists(), "Output directory should be created"

    # Verify log file exists
    log_file = output_dir / "logs" / "pipeline.log"
    assert log_file.exists(), "Log file should be created"

    # Verify all stage outputs exist
    assert (output_dir / "stage1" / "inspirations.json").exists()
    assert (output_dir / "stage2" / "trend-signals.md").exists()
    assert (output_dir / "stage3" / "universal-lessons.md").exists()
    assert (output_dir / "stage4" / "brand-contextualization.md").exists()
    assert (output_dir / "stage5" / "opportunity-1.md").exists()
```

---

#### E2E Test Example (3.2-E2E-003)

```bash
#!/bin/bash
# Test backward compatibility of --batch mode

# Run batch mode
python scripts/run_pipeline.py --batch

# Verify exit code
if [ $? -ne 0 ]; then
    echo "FAIL: Batch mode returned non-zero exit code"
    exit 1
fi

# Verify outputs exist for all brands
for brand in lactalis-canada danone-canada; do
    output_dir="data/test-outputs/batch-$(date +%Y%m%d)/$brand"
    if [ ! -d "$output_dir/stage5" ]; then
        echo "FAIL: Missing stage5 output for $brand"
        exit 1
    fi
done

echo "PASS: Batch mode regression test"
```

---

**End of Test Design Document**
