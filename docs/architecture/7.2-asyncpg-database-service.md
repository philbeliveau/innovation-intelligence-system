# 7.2 asyncpg Database Service (Railway Backend)

## Overview

The Railway backend uses **asyncpg** (Python async PostgreSQL driver) to write pipeline data to the database. This service handles stage-by-stage updates, opportunity card creation, and run status management.

## Integration Mechanism: How Pipeline Calls Database Service

### Architecture Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                       Next.js Frontend (Vercel)                 │
│                                                                  │
│  User clicks "Launch" → POST /api/run                          │
└────────────────────────────────┬────────────────────────────────┘
                                 │
                                 │ HTTP POST
                                 ▼
┌─────────────────────────────────────────────────────────────────┐
│                    FastAPI Backend (Railway)                     │
│                                                                  │
│  POST /run endpoint                                             │
│    │                                                             │
│    ├─→ 1. Create Run record (Prisma or asyncpg)                │
│    │                                                             │
│    └─→ 2. Trigger background task: execute_pipeline_with_updates()
└────────────────────────────────┬────────────────────────────────┘
                                 │
                                 │ Async background execution
                                 ▼
┌─────────────────────────────────────────────────────────────────┐
│              Pipeline Execution (5 Stages)                      │
│                                                                  │
│  Stage 1 Executor                                               │
│    ├─→ Load document from Blob URL                             │
│    ├─→ Run LLM processing (OpenRouter)                         │
│    ├─→ Call db.save_inspiration_report(...)                    │
│    └─→ Call db.update_run_stage(1)                             │
│                                 │                                │
│                                 ▼                                │
│  Stage 2 Executor                                               │
│    ├─→ Read Stage 1 output from database (or file fallback)    │
│    ├─→ Run LLM processing                                       │
│    ├─→ Call db.save_stage_output(stage_number=2, ...)          │
│    └─→ Call db.update_run_stage(2)                             │
│                                 │                                │
│                                 ▼                                │
│  Stage 3, 4, 5... (same pattern)                               │
│                                 │                                │
│                                 ▼                                │
│  Stage 5 Executor                                               │
│    ├─→ Generate 5 opportunity cards                            │
│    ├─→ Call db.save_opportunity_cards(...)                     │
│    └─→ Call db.complete_run()                                  │
└─────────────────────────────────┬────────────────────────────────┘
                                 │
                                 │ Database writes
                                 ▼
┌─────────────────────────────────────────────────────────────────┐
│                   PostgreSQL (Railway)                          │
│                                                                  │
│  Tables: Run, OpportunityCard, InspirationReport, StageOutput  │
└─────────────────────────────────────────────────────────────────┘
```

### Sequence Diagram: Single Pipeline Execution

```
User        Next.js API       FastAPI       Pipeline       DatabaseService    PostgreSQL
 │              │                │              │                 │                │
 │─Upload PDF─→│                │              │                 │                │
 │              │                │              │                 │                │
 │              │─POST /api/run─→│              │                 │                │
 │              │   (blob_url)   │              │                 │                │
 │              │                │              │                 │                │
 │              │                │─INSERT Run──→│─────────────────→│─────────────→│
 │              │                │              │                 │                │
 │              │←─{run_id}──────│              │                 │                │
 │←─{run_id}────│                │              │                 │                │
 │              │                │              │                 │                │
 │              │                │─Background──→│                 │                │
 │              │                │  Task Start  │                 │                │
 │              │                │              │                 │                │
 │              │                │              │─start_run()────→│─UPDATE Run────→│
 │              │                │              │                 │  status=RUNNING│
 │              │                │              │                 │                │
 │              │                │              │─Stage 1 LLM────→│                │
 │              │                │              │  Processing     │                │
 │              │                │              │                 │                │
 │              │                │              │─save_inspiration_report()───────→│
 │              │                │              │                 │─INSERT Insp───→│
 │              │                │              │                 │                │
 │              │                │              │─update_run_stage(1)─────────────→│
 │              │                │              │                 │─UPDATE currentStage│
 │              │                │              │                 │                │
 │              │                │              │─Stage 2 LLM────→│                │
 │              │                │              │                 │                │
 │              │                │              │─save_stage_output(2)────────────→│
 │              │                │              │                 │─INSERT StageOut│
 │              │                │              │                 │                │
 │              │                │              │─update_run_stage(2)─────────────→│
 │              │                │              │                 │                │
 │              │                │              │  ... Stages 3-4 ...              │
 │              │                │              │                 │                │
 │              │                │              │─Stage 5 LLM────→│                │
 │              │                │              │                 │                │
 │              │                │              │─save_opportunity_cards()────────→│
 │              │                │              │                 │─INSERT 5 Cards─→│
 │              │                │              │                 │                │
 │              │                │              │─complete_run()──→│─UPDATE Run────→│
 │              │                │              │                 │ status=COMPLETED│
 │              │                │              │                 │ completedAt=NOW│
 │              │                │              │                 │                │
│─Poll Status─→│─GET /api/status/{run_id}────→│─────────────────→│─SELECT Run────→│
│              │                │              │                 │                │
│←─{status:COMPLETED}───────────│              │                 │                │
```

### Code Integration: Injecting DatabaseService into Pipeline

**File:** `pipeline/orchestrator.py`

```python
import asyncio
from pathlib import Path
from typing import Optional, Dict
from backend.app.services.database_service import DatabaseService
from pipeline.stages.stage1 import Stage1Executor
from pipeline.stages.stage2 import Stage2Executor
from pipeline.stages.stage3 import Stage3Executor
from pipeline.stages.stage4 import Stage4Executor
from pipeline.stages.stage5 import Stage5Executor
import logging

logger = logging.getLogger(__name__)

class PipelineOrchestrator:
    """
    Orchestrates the 5-stage pipeline with database integration.

    This class coordinates:
    1. Stage-by-stage execution
    2. Database writes after each stage
    3. Run status updates
    4. Error handling and rollback
    """

    def __init__(self, db_service: Optional[DatabaseService] = None):
        """
        Initialize orchestrator with optional database service.

        Args:
            db_service: If None, falls back to file-based mode (backward compatible)
        """
        self.db_service = db_service
        self.use_database = db_service is not None

    async def execute_full_pipeline(
        self,
        document_path: str,
        brand_id: str,
        run_id: str,
        user_id: Optional[str] = None
    ) -> Dict:
        """
        Execute all 5 stages with database updates.

        Args:
            document_path: Path to uploaded document (local or Blob URL)
            brand_id: Company ID (e.g., "lactalis-canada")
            run_id: Unique run identifier
            user_id: Clerk user ID (required for database mode)

        Returns:
            Final pipeline result with status
        """

        try:
            # Mark run as RUNNING
            if self.use_database:
                await self.db_service.start_run(run_id)

            logger.info(f"[{run_id}] Starting pipeline execution")

            # --- STAGE 1: Inspiration Extraction ---
            logger.info(f"[{run_id}] Stage 1: Inspiration Extraction")
            if self.use_database:
                await self.db_service.update_run_stage(run_id, 1)

            stage1 = Stage1Executor(db_service=self.db_service)
            stage1_result = await stage1.execute(
                document_path=document_path,
                run_id=run_id
            )

            # Save to database
            if self.use_database:
                await self.db_service.save_inspiration_report(
                    run_id=run_id,
                    inspiration1=stage1_result["inspirations"][0],
                    inspiration2=stage1_result["inspirations"][1]
                )
                await self.db_service.save_stage_output(
                    run_id=run_id,
                    stage_number=1,
                    markdown_output=stage1_result.get("markdown", ""),
                    json_output=stage1_result
                )

            # --- STAGE 2: Trend Amplification ---
            logger.info(f"[{run_id}] Stage 2: Trend Amplification")
            if self.use_database:
                await self.db_service.update_run_stage(run_id, 2)

            stage2 = Stage2Executor(db_service=self.db_service)
            stage2_result = await stage2.execute(
                run_id=run_id,
                previous_output=stage1_result
            )

            if self.use_database:
                await self.db_service.save_stage_output(
                    run_id=run_id,
                    stage_number=2,
                    markdown_output=stage2_result["markdown"],
                    json_output=stage2_result.get("json")
                )

            # --- STAGE 3: Universal Translation ---
            logger.info(f"[{run_id}] Stage 3: Universal Translation")
            if self.use_database:
                await self.db_service.update_run_stage(run_id, 3)

            stage3 = Stage3Executor(db_service=self.db_service)
            stage3_result = await stage3.execute(
                run_id=run_id,
                previous_output=stage2_result
            )

            if self.use_database:
                await self.db_service.save_stage_output(
                    run_id=run_id,
                    stage_number=3,
                    markdown_output=stage3_result["markdown"],
                    json_output=stage3_result.get("json")
                )

            # --- STAGE 4: Brand Contextualization ---
            logger.info(f"[{run_id}] Stage 4: Brand Contextualization")
            if self.use_database:
                await self.db_service.update_run_stage(run_id, 4)

            stage4 = Stage4Executor(db_service=self.db_service)
            stage4_result = await stage4.execute(
                run_id=run_id,
                previous_output=stage3_result,
                brand_id=brand_id
            )

            if self.use_database:
                await self.db_service.save_stage_output(
                    run_id=run_id,
                    stage_number=4,
                    markdown_output=stage4_result["markdown"],
                    json_output=stage4_result.get("json")
                )

            # --- STAGE 5: Opportunity Generation ---
            logger.info(f"[{run_id}] Stage 5: Opportunity Generation")
            if self.use_database:
                await self.db_service.update_run_stage(run_id, 5)

            stage5 = Stage5Executor(db_service=self.db_service)
            stage5_result = await stage5.execute(
                run_id=run_id,
                previous_output=stage4_result,
                brand_id=brand_id
            )

            if self.use_database and user_id:
                # Parse markdown and save 5 opportunity cards
                await self.db_service.save_opportunity_cards(
                    run_id=run_id,
                    user_id=user_id,
                    stage5_output=stage5_result["markdown"]
                )
                await self.db_service.save_stage_output(
                    run_id=run_id,
                    stage_number=5,
                    markdown_output=stage5_result["markdown"],
                    json_output=stage5_result.get("json")
                )

            # Mark run as COMPLETED
            if self.use_database:
                await self.db_service.complete_run(run_id)

            logger.info(f"[{run_id}] Pipeline completed successfully")

            return {
                "status": "COMPLETED",
                "run_id": run_id,
                "current_stage": 5,
                "stage5_result": stage5_result
            }

        except Exception as e:
            logger.error(f"[{run_id}] Pipeline failed: {str(e)}")

            # Mark run as FAILED
            if self.use_database:
                await self.db_service.fail_run(run_id, str(e))

            return {
                "status": "FAILED",
                "run_id": run_id,
                "error_message": str(e)
            }

        finally:
            # Close database connection pool
            if self.use_database:
                await self.db_service.close_pool()
```

### Modified Stage Executors: Database Integration Pattern

**File:** `pipeline/stages/stage1.py` (Example)

```python
import asyncio
from typing import Optional, Dict
from pathlib import Path
import json
from langchain.chat_models import ChatOpenAI
from backend.app.services.database_service import DatabaseService
import logging

logger = logging.getLogger(__name__)

class Stage1Executor:
    """
    Stage 1: Inspiration Extraction

    Extracts 2 innovation inspirations from uploaded document.
    """

    def __init__(self, db_service: Optional[DatabaseService] = None):
        """
        Initialize Stage 1 with optional database service.

        Args:
            db_service: If provided, writes to database; otherwise falls back to files
        """
        self.db_service = db_service
        self.use_database = db_service is not None

        # LLM client initialization
        self.llm = ChatOpenAI(
            model="anthropic/claude-sonnet-4.5",
            openai_api_key=os.getenv("OPENROUTER_API_KEY"),
            openai_api_base=os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        )

    async def execute(self, document_path: str, run_id: str) -> Dict:
        """
        Execute Stage 1 with database integration.

        Args:
            document_path: Path to document (local file or Blob URL)
            run_id: Unique run identifier

        Returns:
            Stage 1 result with inspirations
        """

        # 1. Load document
        document_text = await self._load_document(document_path)

        # 2. Run LLM processing (existing logic)
        result = await self._extract_inspirations(document_text)

        # 3. Write to database (NEW)
        if self.use_database:
            logger.info(f"[{run_id}] Writing Stage 1 output to database")
            await self.db_service.save_inspiration_report(
                run_id=run_id,
                inspiration1=result["inspirations"][0],
                inspiration2=result["inspirations"][1]
            )

        # 4. Write to file (FALLBACK - keep for backward compatibility)
        output_dir = Path(f"data/test-outputs/{run_id}/stage1")
        output_dir.mkdir(parents=True, exist_ok=True)

        with open(output_dir / "inspirations.json", "w") as f:
            json.dump(result, f, indent=2)

        logger.info(f"[{run_id}] Stage 1 completed")

        return result

    async def _load_document(self, document_path: str) -> str:
        """Load document from file or Blob URL."""
        # Implementation: Download from Blob or read local file
        pass

    async def _extract_inspirations(self, text: str) -> Dict:
        """Run LLM to extract 2 inspirations."""
        # Implementation: LLM prompt + parsing
        pass
```

---

## 7.2.1 Database Service Architecture

### File Structure

```
backend/
  app/
    services/
      database_service.py     # Main database service class
    routers/
      pipeline.py             # Pipeline endpoints with DB integration
  requirements.txt            # Add: asyncpg==0.29.0
```

### Connection Pool Management

```python
# backend/app/services/database_service.py

import asyncpg
import os
from typing import Dict, Optional, List
import json
from datetime import datetime

class DatabaseService:
    """Async PostgreSQL service for pipeline writes."""

    def __init__(self):
        self.database_url = os.getenv("DATABASE_URL")
        self._pool: Optional[asyncpg.Pool] = None

    async def get_pool(self) -> asyncpg.Pool:
        """Get or create connection pool."""
        if not self._pool:
            self._pool = await asyncpg.create_pool(
                self.database_url,
                min_size=5,      # Minimum connections
                max_size=20,     # Maximum connections
                command_timeout=60,
                server_settings={
                    'application_name': 'innovation-pipeline'
                }
            )
        return self._pool

    async def close_pool(self):
        """Close connection pool gracefully."""
        if self._pool:
            await self._pool.close()
            self._pool = None
```

---

## 7.2.2 Run Status Management

### Update Run Stage

```python
async def update_run_stage(self, run_id: str, stage: int):
    """Update currentStage field during pipeline execution."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.execute(
            '''
            UPDATE "Run"
            SET "currentStage" = $1, "updatedAt" = NOW()
            WHERE id = $2
            ''',
            stage, run_id
        )
```

### Mark Run as Running

```python
async def start_run(self, run_id: str):
    """Mark run as RUNNING when pipeline starts."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.execute(
            '''
            UPDATE "Run"
            SET status = 'RUNNING', "updatedAt" = NOW()
            WHERE id = $1
            ''',
            run_id
        )
```

### Complete Run

```python
async def complete_run(self, run_id: str):
    """Mark run as COMPLETED with timestamp."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.execute(
            '''
            UPDATE "Run"
            SET
                status = 'COMPLETED',
                "currentStage" = 5,
                "completedAt" = NOW(),
                "updatedAt" = NOW()
            WHERE id = $1
            ''',
            run_id
        )
```

### Fail Run

```python
async def fail_run(self, run_id: str, error_message: str):
    """Mark run as FAILED with error message."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.execute(
            '''
            UPDATE "Run"
            SET
                status = 'FAILED',
                "errorMessage" = $2,
                "updatedAt" = NOW()
            WHERE id = $1
            ''',
            run_id, error_message
        )
```

---

## 7.2.3 Stage Output Storage

### Save Stage Output

```python
async def save_stage_output(
    self,
    run_id: str,
    stage_number: int,
    markdown_output: str,
    json_output: Optional[Dict] = None
):
    """Save stage output to StageOutput table."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.execute(
            '''
            INSERT INTO "StageOutput" (id, "runId", "stageNumber", "markdownOutput", "jsonOutput", "createdAt")
            VALUES (gen_random_uuid(), $1, $2, $3, $4, NOW())
            ''',
            run_id,
            stage_number,
            markdown_output,
            json.dumps(json_output) if json_output else None
        )
```

---

## 7.2.4 Inspiration Report Creation

### Save Inspiration Report (Stage 1)

```python
async def save_inspiration_report(
    self,
    run_id: str,
    inspiration1: Dict,
    inspiration2: Dict
):
    """Save InspirationReport from Stage 1 output."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.execute(
            '''
            INSERT INTO "InspirationReport" (
                id,
                "runId",
                "inspiration1Title",
                "inspiration1Content",
                "inspiration1Elements",
                "inspiration2Title",
                "inspiration2Content",
                "inspiration2Elements",
                "createdAt",
                "updatedAt"
            )
            VALUES (
                gen_random_uuid(),
                $1, $2, $3, $4, $5, $6, $7,
                NOW(), NOW()
            )
            ''',
            run_id,
            inspiration1['title'],
            inspiration1['content'],
            inspiration1['key_elements'],  # PostgreSQL array
            inspiration2['title'],
            inspiration2['content'],
            inspiration2['key_elements']
        )
```

---

## 7.2.5 Opportunity Card Creation

### Parse and Save Cards (Stage 5)

```python
async def save_opportunity_cards(
    self,
    run_id: str,
    user_id: str,
    stage5_output: str
):
    """Parse Stage 5 markdown and save 5 opportunity cards."""
    # Parse markdown to extract cards
    cards = self._parse_opportunity_cards(stage5_output)

    pool = await self.get_pool()
    async with pool.acquire() as conn:
        # Use transaction to ensure all 5 cards are inserted
        async with conn.transaction():
            for card in cards:
                await conn.execute(
                    '''
                    INSERT INTO "OpportunityCard" (
                        id,
                        "runId",
                        "userId",
                        number,
                        title,
                        tagline,
                        problem,
                        solution,
                        "why_now",
                        "target_segment",
                        "quick_win",
                        starred,
                        "createdAt",
                        "updatedAt"
                    )
                    VALUES (
                        gen_random_uuid(),
                        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, false,
                        NOW(), NOW()
                    )
                    ''',
                    run_id,
                    user_id,
                    card['number'],
                    card['title'],
                    card['tagline'],
                    card['problem'],
                    card['solution'],
                    card['why_now'],
                    card['target_segment'],
                    card['quick_win']
                )

def _parse_opportunity_cards(self, markdown: str) -> List[Dict]:
    """Parse Stage 5 markdown output into structured cards."""
    import re

    cards = []

    # Extract each card (1-5)
    for i in range(1, 6):
        pattern = rf'''
        ##\s*Opportunity\s*{i}:?\s*(.+?)\n\n  # Title
        \*\*Tagline:\*\*\s*(.+?)\n\n           # Tagline
        \*\*Problem:\*\*\s*(.+?)\n\n           # Problem
        \*\*Solution:\*\*\s*(.+?)\n\n          # Solution
        \*\*Why\s+Now:\*\*\s*(.+?)\n\n         # Why Now
        \*\*Target\s+Segment:\*\*\s*(.+?)\n\n  # Target Segment
        \*\*Quick\s+Win:\*\*\s*(.+?)(?:\n\n|$) # Quick Win
        '''

        match = re.search(pattern, markdown, re.DOTALL | re.VERBOSE | re.IGNORECASE)

        if match:
            cards.append({
                'number': i,
                'title': match.group(1).strip(),
                'tagline': match.group(2).strip(),
                'problem': match.group(3).strip(),
                'solution': match.group(4).strip(),
                'why_now': match.group(5).strip(),
                'target_segment': match.group(6).strip(),
                'quick_win': match.group(7).strip()
            })
        else:
            # Fallback if parsing fails
            cards.append({
                'number': i,
                'title': f'Opportunity {i}',
                'tagline': 'Unable to parse',
                'problem': 'See full Stage 5 markdown',
                'solution': 'See full Stage 5 markdown',
                'why_now': 'See full Stage 5 markdown',
                'target_segment': 'See full Stage 5 markdown',
                'quick_win': 'See full Stage 5 markdown'
            })

    return cards
```

---

## 7.2.6 Pipeline Execution Integration

### FastAPI Router with Database Updates

```python
# backend/app/routers/pipeline.py

from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel
from app.services.database_service import DatabaseService
from app.services.pipeline_service import PipelineService
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class RunRequest(BaseModel):
    run_id: str
    blob_url: str
    company_id: str
    user_id: str

@router.post("/run")
async def execute_pipeline(
    request: RunRequest,
    background_tasks: BackgroundTasks
):
    """Trigger pipeline execution with database updates."""

    # Start pipeline in background
    background_tasks.add_task(
        execute_pipeline_with_updates,
        request.run_id,
        request.blob_url,
        request.company_id,
        request.user_id
    )

    return {"status": "Pipeline started", "run_id": request.run_id}

async def execute_pipeline_with_updates(
    run_id: str,
    blob_url: str,
    company_id: str,
    user_id: str
):
    """Execute 5-stage pipeline with stage-by-stage database updates."""

    db = DatabaseService()
    pipeline = PipelineService()

    try:
        # Mark run as RUNNING
        await db.start_run(run_id)
        logger.info(f"[{run_id}] Pipeline started")

        # Stage 1: Input Processing
        await db.update_run_stage(run_id, 1)
        logger.info(f"[{run_id}] Starting Stage 1")

        stage1_output = await pipeline.run_stage1(blob_url)

        # Save Stage 1 outputs
        await db.save_stage_output(
            run_id=run_id,
            stage_number=1,
            markdown_output=stage1_output['markdown'],
            json_output=stage1_output['json']
        )

        # Save inspiration report
        await db.save_inspiration_report(
            run_id=run_id,
            inspiration1=stage1_output['json']['inspiration_1'],
            inspiration2=stage1_output['json']['inspiration_2']
        )

        logger.info(f"[{run_id}] Stage 1 completed")

        # Stage 2: Signal Amplification
        await db.update_run_stage(run_id, 2)
        logger.info(f"[{run_id}] Starting Stage 2")

        stage2_output = await pipeline.run_stage2(stage1_output['markdown'], company_id)

        await db.save_stage_output(
            run_id=run_id,
            stage_number=2,
            markdown_output=stage2_output
        )

        logger.info(f"[{run_id}] Stage 2 completed")

        # Stage 3: General Translation
        await db.update_run_stage(run_id, 3)
        logger.info(f"[{run_id}] Starting Stage 3")

        stage3_output = await pipeline.run_stage3(stage2_output)

        await db.save_stage_output(
            run_id=run_id,
            stage_number=3,
            markdown_output=stage3_output
        )

        logger.info(f"[{run_id}] Stage 3 completed")

        # Stage 4: Brand Contextualization
        await db.update_run_stage(run_id, 4)
        logger.info(f"[{run_id}] Starting Stage 4")

        stage4_output = await pipeline.run_stage4(stage3_output, company_id)

        await db.save_stage_output(
            run_id=run_id,
            stage_number=4,
            markdown_output=stage4_output
        )

        logger.info(f"[{run_id}] Stage 4 completed")

        # Stage 5: Opportunity Generation
        await db.update_run_stage(run_id, 5)
        logger.info(f"[{run_id}] Starting Stage 5")

        stage5_output = await pipeline.run_stage5(stage4_output, company_id)

        await db.save_stage_output(
            run_id=run_id,
            stage_number=5,
            markdown_output=stage5_output
        )

        # Parse and save opportunity cards
        await db.save_opportunity_cards(
            run_id=run_id,
            user_id=user_id,
            stage5_output=stage5_output
        )

        logger.info(f"[{run_id}] Stage 5 completed")

        # Mark run as COMPLETED
        await db.complete_run(run_id)
        logger.info(f"[{run_id}] Pipeline completed successfully")

    except Exception as e:
        # Mark run as FAILED
        error_message = str(e)
        await db.fail_run(run_id, error_message)
        logger.error(f"[{run_id}] Pipeline failed: {error_message}")
        raise

    finally:
        # Close connection pool
        await db.close_pool()
```

---

## 7.2.7 Error Handling & Retry Logic

### Connection Pool Error Handling

```python
async def get_pool(self) -> asyncpg.Pool:
    """Get or create connection pool with retry logic."""
    if not self._pool:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self._pool = await asyncpg.create_pool(
                    self.database_url,
                    min_size=5,
                    max_size=20,
                    command_timeout=60
                )
                return self._pool
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                logger.warning(f"Connection pool creation failed (attempt {attempt + 1}): {e}")
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
    return self._pool
```

### Transaction Rollback on Failure

```python
async def save_opportunity_cards(self, run_id: str, user_id: str, stage5_output: str):
    """Save all 5 cards in a transaction (all-or-nothing)."""
    cards = self._parse_opportunity_cards(stage5_output)

    pool = await self.get_pool()
    async with pool.acquire() as conn:
        try:
            async with conn.transaction():
                for card in cards:
                    await conn.execute(INSERT_QUERY, ...)
        except Exception as e:
            logger.error(f"Failed to save opportunity cards for run {run_id}: {e}")
            raise  # Transaction automatically rolls back
```

---

## 7.2.8 Performance Optimization

### Batch Inserts for Stage Outputs

```python
async def save_multiple_stage_outputs(self, outputs: List[Dict]):
    """Batch insert multiple stage outputs."""
    pool = await self.get_pool()
    async with pool.acquire() as conn:
        await conn.executemany(
            '''
            INSERT INTO "StageOutput" (id, "runId", "stageNumber", "markdownOutput", "createdAt")
            VALUES (gen_random_uuid(), $1, $2, $3, NOW())
            ''',
            [(o['run_id'], o['stage_number'], o['markdown']) for o in outputs]
        )
```

### Connection Pool Monitoring

```python
async def get_pool_stats(self) -> Dict:
    """Get connection pool statistics."""
    if not self._pool:
        return {"status": "not_initialized"}

    return {
        "size": self._pool.get_size(),
        "min_size": self._pool.get_min_size(),
        "max_size": self._pool.get_max_size(),
        "idle": self._pool.get_idle_size()
    }
```

---

## 7.2.9 Database Schema Compatibility

### Prisma Schema → asyncpg SQL

The asyncpg service uses direct SQL queries that match the Prisma schema:

| Prisma Model | PostgreSQL Table | asyncpg Query Example |
|--------------|------------------|----------------------|
| `Run` | `"Run"` | `UPDATE "Run" SET status = $1 WHERE id = $2` |
| `OpportunityCard` | `"OpportunityCard"` | `INSERT INTO "OpportunityCard" (...) VALUES (...)` |
| `InspirationReport` | `"InspirationReport"` | `INSERT INTO "InspirationReport" (...) VALUES (...)` |
| `StageOutput` | `"StageOutput"` | `INSERT INTO "StageOutput" (...) VALUES (...)` |

**Important:** Table and column names are **case-sensitive** and match Prisma's naming convention (double-quoted in SQL).

---

## 7.2.10 Testing Database Service

### Unit Test Example

```python
# backend/tests/test_database_service.py

import pytest
from app.services.database_service import DatabaseService

@pytest.mark.asyncio
async def test_update_run_stage():
    db = DatabaseService()

    # Create test run
    run_id = "test-run-uuid"

    # Update stage
    await db.update_run_stage(run_id, 3)

    # Verify update (requires test database)
    pool = await db.get_pool()
    async with pool.acquire() as conn:
        row = await conn.fetchrow(
            'SELECT "currentStage" FROM "Run" WHERE id = $1',
            run_id
        )
        assert row['currentStage'] == 3

    await db.close_pool()
```

---

## Summary

### Key Capabilities

| Operation | Method | Purpose |
|-----------|--------|---------|
| `start_run()` | Update run → RUNNING | Mark pipeline started |
| `update_run_stage()` | Update currentStage | Track progress (0-5) |
| `save_stage_output()` | Insert StageOutput | Save markdown + JSON |
| `save_inspiration_report()` | Insert InspirationReport | Save Stage 1 inspirations |
| `save_opportunity_cards()` | Insert 5 OpportunityCards | Save Stage 5 cards |
| `complete_run()` | Update run → COMPLETED | Mark success |
| `fail_run()` | Update run → FAILED | Mark failure with error |

### Architecture Benefits

✅ **Async Performance** - High-throughput connection pooling
✅ **Transaction Support** - ACID guarantees for multi-step operations
✅ **Error Recovery** - Automatic rollback on failure
✅ **Connection Pooling** - 5-20 connections for concurrent pipeline runs
✅ **Type Safety** - Prisma schema enforces database structure

---
