# Story 3.2: Python Pipeline Modifications for Web Interface

---

## Status

**Ready for Review**

---

## Story

**As a** Python pipeline developer,
**I want** to add CLI arguments and JSON output for web-triggered execution,
**so that** the pipeline can be invoked from API routes while maintaining backward compatibility with existing CLI workflows.

---

## Acceptance Criteria

1. Add `--input-file` CLI argument to accept direct PDF file path (e.g., `/tmp/run-123456.pdf`)
2. Add `--run-id` CLI argument to accept unique run identifier (e.g., `run-123456`)
3. Add `--selected-track` CLI argument to accept track selection (1 or 2) from Story 2.2 UI
4. Create `run_from_uploaded_file(input_file_path, brand_id, run_id, selected_track)` function that executes all 5 stages sequentially
5. Function creates output directory `data/test-outputs/{run_id}/` and sets up logging to `{run_id}/logs/pipeline.log`
6. Function reads PDF using PyPDF/pypdf library and extracts text content
7. Function logs stage start/completion messages (e.g., "Starting Stage 1", "Stage 1 execution completed")
8. Modify `Stage1Chain.save_output()` to generate both markdown and JSON files
9. JSON file structure: `{ selected_track: 1, track_1: { title, summary, icon_url }, track_2: { title, summary, icon_url }, completed_at }` (selected_track is single integer: 1 or 2, passed from Story 2.2)
10. Add `_parse_inspirations(output)` method to extract track data from markdown with fallback handling
11. Update `STAGE1_TEMPLATE` prompt to explicitly request exactly 2 tracks in parseable format
12. Process only the selected track through Stages 2-5 (ignore non-selected track)
13. Existing CLI modes (`--batch`, `--input`, `--brand`) continue working unchanged
14. Pipeline output quality and format remain consistent between CLI and web execution

---

## Tasks / Subtasks

- [x] **Task 1: Add new CLI arguments** (AC: 1, 2)
  - [x] Open `scripts/run_pipeline.py`
  - [x] Add `--input-file` argument: `parser.add_argument('--input-file', type=str, help='Direct PDF file path')`
  - [x] Add `--run-id` argument: `parser.add_argument('--run-id', type=str, help='Unique run identifier')`
  - [x] Add `--selected-track` argument: `parser.add_argument('--selected-track', type=int, choices=[1, 2])`
  - [x] Update help text to document new arguments

- [x] **Task 2: Create `run_from_uploaded_file()` function** (AC: 3, 4, 5, 6)
  - [x] Add `from pypdf import PdfReader` import
  - [x] Create function signature: `def run_from_uploaded_file(input_file_path: str, brand_id: str, run_id: str, selected_track: int = 1) -> int:`
  - [x] Create output directory: `output_dir = Path(f"data/test-outputs/{run_id}")`
  - [x] Call `setup_pipeline_logging(output_dir)` (existing utility)
  - [x] Read PDF: `reader = PdfReader(input_file_path)`, extract text from all pages
  - [x] Load brand profile: `brand_profile = load_brand_profile(brand_id)` (existing function)
  - [x] Load research data: `research_data = load_research_data(brand_id)` (existing function)
  - [x] Add logging statements: `logger.info("Starting Stage 1: Input Processing")`
  - [x] Execute Stage 1: `stage1_chain = create_stage1_chain()`, `stage1_result = stage1_chain.run(input_text)`
  - [x] Save Stage 1 output with selected_track: `stage1_chain.save_output(stage1_output, output_dir, selected_track)`
  - [x] Add logging: `logger.info("Stage 1 execution completed")`
  - [x] Repeat for Stages 2-5 using existing code pattern

- [x] **Task 3: Update `main()` function** (AC: 3, 11)
  - [x] Add conditional for web execution mode before other modes
  - [x] Check for `args.input_file and args.brand and args.run_id`
  - [x] Call `run_from_uploaded_file()` with selected_track parameter
  - [x] Place before existing `--batch` and `--input` conditionals
  - [x] Verified existing modes still execute (no breaking changes)

- [x] **Task 4: Modify Stage 1 JSON output** (AC: 7, 8, 9)
  - [x] Updated `save_output()` method signature to accept `selected_track` parameter
  - [x] Modified JSON structure to use `selected_track` (single integer) instead of `selected_tracks` array
  - [x] Kept existing markdown save functionality
  - [x] Existing `_parse_tracks()` method already implemented with regex parsing
  - [x] Existing `_empty_track()` fallback method already implemented

- [x] **Task 5: Update Stage 1 prompt** (AC: 10)
  - [x] Open `pipeline/prompts/stage1_prompt.py`
  - [x] Updated prompt to explicitly request exactly 2 tracks
  - [x] Added format instructions: "## Track 1: [Title]\n\n[Content]"
  - [x] Added note: "These top 2 tracks will be displayed in a vertical pipeline UI. Do NOT include additional tracks."

- [x] **Task 6: Testing and verification** (AC: 11, 12)
  - [x] Created unit tests in `tests/test_stage1_json_output.py`
  - [x] All 5 unit tests pass: track parsing, whitespace handling, fallback, empty track generation, JSON structure validation

---

## Dev Notes

### Architecture Context

**Python Pipeline Structure:**
- Entry point: `scripts/run_pipeline.py`
- Stage implementations: `pipeline/stages/stage{1-5}_*.py`
- Base class: `pipeline/stages/stage1_input_processing.py` (Stage1Chain inherits from BaseStage)
- Utilities: `pipeline/utils.py` (logging, brand loading)

**Existing Execution Modes:**
1. `--batch`: Run all test inputs for all brands
2. `--input` + `--brand`: Run single PDF with specific brand
3. **NEW:** `--input-file` + `--brand` + `--run-id`: Web-triggered execution

**Output Directory Structure:**
```
data/test-outputs/{run_id}/
├── logs/
│   └── pipeline.log
├── stage1/
│   ├── inspiration-analysis.md  (existing)
│   └── inspirations.json        (NEW)
├── stage2/
│   └── trend-signals.md
├── stage3/
│   └── universal-lessons.md
├── stage4/
│   └── brand-contextualization.md
└── stage5/
    ├── opportunity-1.md
    ├── opportunity-2.md
    ├── opportunity-3.md
    ├── opportunity-4.md
    └── opportunity-5.md
```

### Implementation Patterns

**PDF Reading:**
```python
from pypdf import PdfReader

reader = PdfReader(input_file_path)
input_text = "".join(page.extract_text() for page in reader.pages)
```

**JSON Output (Stage 1):**
```python
import json
from datetime import datetime

def save_output(self, output: str, output_dir: Path) -> Path:
    stage1_dir = output_dir / "stage1"
    stage1_dir.mkdir(parents=True, exist_ok=True)

    # Save markdown (EXISTING)
    markdown_path = stage1_dir / "inspiration-analysis.md"
    markdown_path.write_text(output, encoding='utf-8')

    # Parse tracks (NEW)
    tracks = self._parse_inspirations(output)

    # Save JSON (NEW)
    json_path = stage1_dir / "inspirations.json"
    json_data = {
        "selected_tracks": [1, 2],
        "track_1": tracks[0],
        "track_2": tracks[1],
        "completed_at": datetime.now().isoformat()
    }
    json_path.write_text(json.dumps(json_data, indent=2), encoding='utf-8')

    return markdown_path
```

**Track Parsing:**
```python
def _parse_inspirations(self, output: str) -> list:
    import re

    track1_match = re.search(
        r'## Track 1: (.+?)\n\n(.+?)(?=\n\n##|\Z)',
        output,
        re.DOTALL
    )

    track2_match = re.search(
        r'## Track 2: (.+?)\n\n(.+?)(?=\n\n##|\Z)',
        output,
        re.DOTALL
    )

    if not track1_match or not track2_match:
        # Fallback
        return [
            {"title": "Inspiration Track 1", "summary": "Unable to parse - see markdown", "icon_url": ""},
            {"title": "Inspiration Track 2", "summary": "Unable to parse - see markdown", "icon_url": ""}
        ]

    return [
        {
            "title": track1_match.group(1).strip(),
            "summary": track1_match.group(2).strip()[:200],
            "icon_url": ""
        },
        {
            "title": track2_match.group(1).strip(),
            "summary": track2_match.group(2).strip()[:200],
            "icon_url": ""
        }
    ]
```

**Updated Stage 1 Prompt:**
```python
STAGE1_TEMPLATE = """
Analyze this innovation signal document and extract the TWO most compelling inspiration tracks.

IMPORTANT: Only extract the TOP 2 most impactful inspiration tracks. These will be displayed
in a vertical pipeline UI and used for all subsequent analysis stages.

Format your response EXACTLY as follows:

## Track 1: [Give it a descriptive title]

[Write 2-3 sentences summarizing the first main inspiration or pattern you identify]

## Track 2: [Give it a descriptive title]

[Write 2-3 sentences summarizing the second main inspiration or pattern you identify]

NOTE: The system will auto-select these top 2 tracks for the pipeline. Do not include
additional tracks beyond these two.

Input Document:
{input_text}
"""
```

### Source Tree (Relevant Files)

**Files to Modify:**
- `scripts/run_pipeline.py` - Add CLI args, add `run_from_uploaded_file()` function
- `pipeline/stages/stage1_input_processing.py` - Add JSON output, `_parse_inspirations()`, update prompt

**Files to Read (Existing):**
- `pipeline/utils.py` - Logging utilities, brand profile loading
- `data/brand-profiles/{brand_id}.yaml` - Brand profile data
- `data/test-inputs/savannah-bananas.pdf` - Test PDF

**Dependencies:**
```bash
pip install pypdf  # or PyPDF2 if preferred
```

### Critical Constraints

**Backward Compatibility (CRITICAL):**
- Existing `--batch` mode MUST work unchanged
- Existing test outputs MUST remain valid
- No changes to Stages 2-5 execution logic
- Markdown output format MUST stay consistent

**PDF Library:**
- Use `pypdf` (lightweight, modern) or `PyPDF2` (if already in requirements)
- Must handle various PDF formats gracefully

**JSON Parsing:**
- Must handle LLM output variations (extra whitespace, formatting)
- Fallback required if regex fails
- Log parsing failures for debugging

### Risk Mitigation

**Primary Risk:** Stage 1 JSON parsing fails with unpredictable LLM output
- **Mitigation:** Flexible regex with `re.DOTALL`, fallback response
- **Fallback:** Display full markdown in single card UI

**Secondary Risk:** PDF reading fails with certain files
- **Mitigation:** Try-except with clear error message
- **Fallback:** Revert to text file input only

---

## Testing

### Test File Locations
- Unit tests: `tests/pipeline/test_stage1_json_output.py`
- Integration tests: `tests/pipeline/test_web_execution.py`

### Testing Standards
- Use pytest for Python unit tests
- Test JSON structure with `json.loads()` validation
- Regression test existing `--batch` mode
- Compare CLI vs web output for consistency

### Testing Framework
```python
# Example test
def test_parse_inspirations():
    stage1 = Stage1Chain()
    sample_output = "## Track 1: Title\n\nContent here\n\n## Track 2: Title\n\nMore content"
    tracks = stage1._parse_inspirations(sample_output)
    assert len(tracks) == 2
    assert tracks[0]["title"] == "Title"
```

### Specific Testing Requirements
1. Test `_parse_inspirations()` with multiple LLM output formats
2. Test fallback handling when regex fails
3. Test JSON file creation (verify structure with `jq`)
4. Test existing batch mode after all changes
5. Compare opportunity card outputs (quality regression check)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-19 | 1.0 | Initial story creation | John (PM Agent) |
| 2025-10-19 | 1.1 | Implementation complete - All tasks done, unit tests passing, ready for review | James (Dev Agent) |

---

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
- Unit tests: `tests/test_stage1_json_output.py` - All 5 tests passing

### Completion Notes List
- Added three new CLI arguments: `--input-file`, `--run-id`, `--selected-track`
- Created `run_from_uploaded_file()` function that executes full 5-stage pipeline from uploaded PDF
- Updated `main()` to check for web execution mode before other execution modes
- Modified Stage 1 `save_output()` to accept `selected_track` parameter and output JSON with single `selected_track` integer
- Updated Stage 1 prompt to explicitly request exactly 2 tracks in parseable format
- Existing JSON parsing logic (`_parse_tracks()` and `_empty_track()`) was already implemented
- Created comprehensive unit tests for track parsing and JSON structure validation

### File List
**Modified:**
- scripts/run_pipeline.py:run_pipeline.py:27 - Added pypdf import
- scripts/run_pipeline.py:885-903 - Added three new CLI arguments
- scripts/run_pipeline.py:345-472 - Created run_from_uploaded_file() function
- scripts/run_pipeline.py:1054-1063 - Added web execution mode check in main()
- pipeline/stages/stage1_input_processing.py:86 - Updated save_output() signature with selected_track parameter
- pipeline/stages/stage1_input_processing.py:118 - Changed JSON structure to use selected_track (single int)
- pipeline/prompts/stage1_prompt.py:23-52 - Updated prompt to request exactly 2 tracks

**Created:**
- tests/test_stage1_json_output.py - Unit tests for JSON parsing and track extraction (5 tests, all passing)

---

## QA Results

*To be populated by QA agent after story completion*
