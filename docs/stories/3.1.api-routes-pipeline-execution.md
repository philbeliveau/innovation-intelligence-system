# Story 3.1: API Routes for Pipeline Execution

---

## Status

**Draft**

---

## Story

**As a** web application user,
**I want** to trigger the Python pipeline from the web interface and check its execution status,
**so that** I can process uploaded documents and monitor progress in real-time without using the CLI.

---

## Acceptance Criteria

### Pipeline Execution API (Primary Focus)

1. POST `/api/run` endpoint accepts JSON payload with `blob_url` and `upload_id`, downloads PDF from Vercel Blob, reads company ID from cookie, and executes Python pipeline in background subprocess
2. POST `/api/run` returns JSON response `{ run_id, status: "running" }` immediately (non-blocking execution)
3. Subprocess execution uses non-blocking `exec()` to avoid Vercel timeout
4. Error handling logs subprocess errors to console without crashing API route

### Status Monitoring API

5. GET `/api/status/[runId]` endpoint reads log file from `data/test-outputs/{runId}/logs/pipeline.log` and parses current stage (0-5)
6. GET `/api/status/[runId]` returns JSON: `{ run_id, status, current_stage, stage1_data? }` with Stage 1 JSON data when available
7. Stage detection function correctly identifies pipeline progress from log messages

### Company Context API

8. GET `/api/onboarding/current-company` reads company ID from cookie and returns company data
9. POST `/api/onboarding/set-company` creates HTTP-only cookie with company ID (implemented in Story 1.1 - verify compatibility)

### Quality & Compatibility

10. All endpoints return appropriate HTTP status codes (400 for validation errors, 404 for missing resources, 500 for server errors)
11. Existing Python CLI execution (`--batch` mode) continues working unchanged after integration

### Dependencies from Previous Stories

**Note:** Story 2.1's `/api/analyze-document` endpoint is already implemented and not part of this story.

---

## Tasks / Subtasks

- [ ] **Task 1: Create `/api/run` endpoint** (AC: 1, 2, 7)
  - [ ] Create `app/api/run/route.ts` file
  - [ ] Add request validation for `blob_url` and `upload_id`
  - [ ] Read company ID from HTTP-only cookie
  - [ ] Download PDF from Vercel Blob to `/tmp/{run_id}.pdf`
  - [ ] Generate unique `run_id` using `Date.now()`
  - [ ] Execute Python subprocess: `python scripts/run_pipeline.py --input-file /tmp/{run_id}.pdf --brand {company_id} --run-id {run_id}`
  - [ ] Use `child_process.exec()` (non-blocking)
  - [ ] Log subprocess stdout/stderr to console
  - [ ] Return JSON: `{ run_id, status: "running" }`

- [ ] **Task 2: Create `/api/status/[runId]` endpoint** (AC: 5, 6, 7, 10)
  - [ ] Create `app/api/status/[runId]/route.ts` file
  - [ ] Extract `runId` from URL parameter
  - [ ] Read log file: `data/test-outputs/{runId}/logs/pipeline.log`
  - [ ] Implement `detectStageFromLog()` function (parse for "Starting Stage X" and "Stage X execution completed")
  - [ ] Read Stage 1 JSON if `current_stage >= 1`: `data/test-outputs/{runId}/stage1/inspirations.json`
  - [ ] Return JSON: `{ run_id, status, current_stage, stage1_data }`
  - [ ] Return 404 if log file doesn't exist
  - [ ] Return status "error" if log contains error indicators

- [ ] **Task 3: Create onboarding API routes** (AC: 8, 9)
  - [ ] Create `app/api/onboarding/current-company/route.ts`
  - [ ] Read company ID from cookie using `cookies().get('company_id')`
  - [ ] Load brand profile YAML from `data/brand-profiles/{company_id}.yaml`
  - [ ] Return `{ company_id, company_name }` or 401 error if not set
  - [ ] **Note:** `POST /api/onboarding/set-company` already exists from Story 1.1
  - [ ] Verify compatibility with Story 1.1's cookie implementation

- [ ] **Task 4: Error handling and validation** (AC: 4, 10)
  - [ ] Add try-catch blocks to all endpoints
  - [ ] Return proper error responses with JSON `{ error: string }`
  - [ ] Log all errors to console with context
  - [ ] Add input validation for all request bodies
  - [ ] Handle missing files gracefully (return error, don't crash)

- [ ] **Task 5: Testing and verification** (AC: 11)
  - [ ] Test `/api/run` with Postman/curl (verify run_id returned)
  - [ ] Test `/api/status` with existing run ID (verify stage detection)
  - [ ] Test `/api/status` with non-existent run ID (verify 404)
  - [ ] Verify `/api/analyze-document` from Story 2.1 works (integration test)
  - [ ] Run existing batch mode: `python scripts/run_pipeline.py --batch`
  - [ ] Verify all 4 test runs complete successfully (no regression)

---

## Dev Notes

### Architecture Context

**API Routes Location:** `app/api/` directory (Next.js 15 App Router pattern)

**File-Based Communication:**
- Pipeline writes logs to: `data/test-outputs/{run_id}/logs/pipeline.log`
- Stage 1 writes JSON to: `data/test-outputs/{run_id}/stage1/inspirations.json`
- API routes read these files to provide status updates

**Python Pipeline Integration:**
- Entry point: `scripts/run_pipeline.py`
- New CLI arguments (from Story 3.2): `--input-file`, `--run-id`
- Existing CLI arguments: `--batch`, `--input`, `--brand`
- Pipeline must run unchanged from both CLI and web interface

**Environment Variables (from `.env.local`):**
```
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
LLM_MODEL=anthropic/claude-sonnet-4.5
BLOB_READ_WRITE_TOKEN=vercel_blob_...
```

### Implementation Patterns

**Subprocess Execution:**
```typescript
import { exec } from 'child_process'

const command = `cd ${process.cwd()} && python scripts/run_pipeline.py --input-file ${tmpPath} --brand ${brand_id} --run-id ${run_id}`

exec(command, (error, stdout, stderr) => {
  if (error) console.error(`Pipeline error for ${run_id}:`, error)
  console.log(`Pipeline stdout: ${stdout}`)
  console.error(`Pipeline stderr: ${stderr}`)
})
```

**Log Parsing Function:**
```typescript
function detectStageFromLog(logContent: string): number {
  if (logContent.includes('Stage 5 execution completed')) return 5
  if (logContent.includes('Starting Stage 5')) return 5
  if (logContent.includes('Stage 4 execution completed')) return 4
  if (logContent.includes('Starting Stage 4')) return 4
  if (logContent.includes('Stage 3 execution completed')) return 3
  if (logContent.includes('Starting Stage 3')) return 3
  if (logContent.includes('Stage 2 execution completed')) return 2
  if (logContent.includes('Starting Stage 2')) return 2
  if (logContent.includes('Stage 1 execution completed')) return 1
  if (logContent.includes('Starting Stage 1')) return 1
  return 0
}
```

**LLM Integration:**
```typescript
import { ChatOpenAI } from '@langchain/openai'

const llm = new ChatOpenAI({
  modelName: process.env.LLM_MODEL || 'anthropic/claude-sonnet-4.5',
  openAIApiKey: process.env.OPENROUTER_API_KEY,
  configuration: {
    baseURL: 'https://openrouter.ai/api/v1'
  }
})

const result = await llm.invoke(analysisPrompt)
const analysis = JSON.parse(result.content as string)
```

**Error Response Pattern:**
```typescript
import { NextResponse } from 'next/server'

return NextResponse.json(
  { error: 'Invalid file type' },
  { status: 400 }
)
```

### Source Tree (Relevant Files)

**Files to Create:**
- `app/api/run/route.ts` - Pipeline execution endpoint
- `app/api/status/[runId]/route.ts` - Status polling endpoint
- `app/api/onboarding/current-company/route.ts` - Company context retrieval

**Files Already Implemented (Dependencies):**
- `app/api/onboarding/set-company/route.ts` - Created in Story 1.1
- `app/api/analyze-document/route.ts` - Created in Story 2.1
- `app/api/upload/route.ts` - Created in Story 1.2

**Files to Read (Existing):**
- `scripts/run_pipeline.py` - Pipeline entry point (modified in Story 3.2)
- `data/brand-profiles/{company_id}.yaml` - Brand profile data
- `data/test-outputs/{run_id}/logs/pipeline.log` - Log file for status
- `data/test-outputs/{run_id}/stage1/inspirations.json` - Stage 1 output (from Story 3.2)

**Dependencies (Already Installed):**
- `pdf-parse` - Installed in Story 2.1
- `@langchain/openai` - Installed in Story 2.1
- `yaml` - Installed in Story 1.1

### Critical Constraints

**Vercel Deployment:**
- API routes timeout after 300 seconds (5 minutes)
- Must use non-blocking subprocess execution
- Only `/tmp` and project directory are writable
- Python 3.10+ must be available in deployment environment

**Backward Compatibility:**
- Python CLI must work unchanged
- Existing test outputs must remain valid
- No changes to Stages 2-5 (only Stage 1 gets JSON output in Story 3.2)

### Risk Mitigation

**Primary Risk:** Python subprocess execution fails on Vercel
- **Mitigation:** Use non-blocking `exec()`, test early
- **Fallback:** Run pipeline on external server, trigger via webhook

**Secondary Risk:** Log parsing fails to detect stages
- **Mitigation:** Add explicit status markers in Python logging
- **Fallback:** Write `status.json` file from Python

---

## Testing

### Test File Locations
- Integration tests: `__tests__/api/` directory
- Unit tests for utilities: `__tests__/utils/` directory

### Testing Standards
- Use Jest for unit/integration tests
- Use Postman/curl for manual API testing
- Test all error scenarios (400, 404, 500)
- Verify no regression in Python CLI (`--batch` mode)

### Testing Framework
```typescript
// Example test structure
describe('POST /api/run', () => {
  it('should return run_id and status running', async () => {
    // Test implementation
  })

  it('should return 400 for missing blob_url', async () => {
    // Test implementation
  })
})
```

### Specific Testing Requirements
1. Test subprocess execution (verify non-blocking)
2. Test log file parsing with sample logs
3. Test Stage 1 JSON reading with missing file
4. Test LLM extraction with actual PDF
5. Regression test: Run `python scripts/run_pipeline.py --batch` after API implementation

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-19 | 1.0 | Initial story creation | John (PM Agent) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent*

### Debug Log References
*To be populated by dev agent*

### Completion Notes List
*To be populated by dev agent*

### File List
*To be populated by dev agent*

---

## QA Results

*To be populated by QA agent after story completion*
