# Story 9.3: switch-to-groq-llm

## Status
Draft

## Story
**As a** user analyzing innovation documents,
**I want** the pipeline to execute faster without sacrificing output quality,
**so that** I can analyze more documents in less time

## Acceptance Criteria

1. Pipeline completes in 30-50 seconds (down from 90-130s)
2. Output quality remains equivalent to current DeepSeek Chat results
3. No code changes required (environment variable only)
4. Staging environment tested before production rollout
5. Cost analysis confirms acceptable budget impact

## Tasks / Subtasks

- [ ] Configure Groq environment variables (AC: 3)
  - [ ] Update `LLM_MODEL` from `deepseek/deepseek-chat` to `groq/llama-3.1-70b-versatile`
  - [ ] Update `OPENROUTER_BASE_URL` to `https://api.groq.com/openai/v1`
  - [ ] Verify Groq API key is configured in Railway environment
- [ ] Staging environment testing (AC: 4)
  - [ ] Deploy to Railway staging with Groq configuration
  - [ ] Run 5 test documents through pipeline
  - [ ] Measure actual latency improvement
  - [ ] Verify JSON parsing still works correctly
- [ ] Output quality validation (AC: 2)
  - [ ] Compare DeepSeek vs Groq outputs side-by-side for 5 test documents
  - [ ] Validate opportunity card quality with business stakeholders
  - [ ] Check for any format differences in JSON responses
  - [ ] Verify all 5 opportunity cards generate completely
- [ ] Cost analysis (AC: 5)
  - [ ] Calculate cost per pipeline run with Groq
  - [ ] Compare with current DeepSeek costs
  - [ ] Project monthly cost based on expected usage
  - [ ] Get stakeholder approval on cost increase (if any)
- [ ] Production rollout (AC: 1, 4)
  - [ ] Update Railway production environment variables
  - [ ] Monitor first 10 production pipeline runs
  - [ ] Verify 30-50 second completion time achieved
  - [ ] Document rollback procedure

## Dev Notes

**Relevant Source Tree:**
- `backend/pipeline/utils.py` - LLM configuration (lines 16-64)
- `backend/.env` - Environment variable template
- Railway Dashboard → innovation-backend → Settings → Variables

**Current LLM Configuration:**
```python
# backend/pipeline/utils.py
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek/deepseek-chat")
OPENROUTER_BASE_URL = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
```

**New Configuration (Environment Variables Only):**
```bash
# Railway Environment Variables
LLM_MODEL=groq/llama-3.1-70b-versatile
OPENROUTER_BASE_URL=https://api.groq.com/openai/v1
OPENROUTER_API_KEY=<existing-key>
```

**Key Implementation Notes:**
- **Zero code changes required** - configuration only
- Groq provides 2-3x faster inference than DeepSeek
- Groq uses Llama 3.1 70B model (comparable quality to DeepSeek Chat)
- JSON parsing may differ slightly - requires validation
- Rollback is instant (revert environment variables)

**Important Constraints:**
- Must use OpenRouter-compatible API (Groq supports this)
- Existing prompt engineering should work unchanged
- Temperature and other parameters remain the same
- Max tokens configuration stays at current levels

**Risk Mitigation:**
- Primary Risk: Groq may format JSON responses differently, breaking parsing
- Mitigation: Extensive testing with 20+ documents before production
- Rollback: Change environment variables back to DeepSeek (takes 30 seconds)

**Performance Expectations:**
- Current: 90-130 seconds end-to-end
- Target: 30-50 seconds end-to-end
- Expected improvement: 40-60 seconds saved (50-65% faster)

### Testing

**Test File Location:**
- No new test files required (configuration change only)
- `backend/tests/test_pipeline_integration.py` (run existing tests)

**Testing Standards:**
- Run full pipeline integration tests with Groq configuration
- Compare outputs using existing test documents
- Manual quality review by business stakeholders

**Testing Frameworks:**
- pytest for automated testing
- Manual testing for quality comparison

**Specific Testing Requirements:**
- **Staging Tests:** Run 5 diverse test documents (short, long, technical, consumer-focused, international)
- **JSON Validation:** Verify all stages parse JSON correctly with Groq
- **Quality Review:** Business stakeholder approval on output quality
- **Performance Test:** Measure actual latency with timing logs
- **Cost Test:** Track token usage and calculate costs per run

**Test Documents to Use:**
- Short report (5 pages)
- Long report (20+ pages)
- Technical/B2B focused report
- Consumer-focused report
- International market report

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-28 | 1.0 | Initial story creation from Epic 9 | John (PM) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
