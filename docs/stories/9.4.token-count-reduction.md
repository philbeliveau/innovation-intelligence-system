# Story 9.4: token-count-reduction

## Status
Draft

## Story
**As a** product owner,
**I want** to optimize LLM token usage to reduce costs and improve speed without losing output quality,
**so that** the pipeline runs faster and more cost-effectively at scale

## Acceptance Criteria

1. Total token count reduced from 17,000 to 12,300 (28% reduction)
2. Pipeline saves 10-15 seconds end-to-end
3. Output quality maintained (no truncated insights)
4. Stage 5 still generates all 5 opportunity cards completely
5. Regression tests pass with reduced token counts

## Tasks / Subtasks

- [x] Update Stage 1 token configuration (AC: 1)
  - [x] Reduce max_tokens from 2,500 to 1,800
  - [x] Update prompt to emphasize conciseness
  - [ ] Test with 5 documents to verify extraction quality
- [x] Update Stage 2 token configuration (AC: 1)
  - [x] Reduce max_tokens from 3,000 to 2,200
  - [x] Refine prompt for focused amplification
  - [ ] Test signal amplification completeness
- [x] Update Stage 5 token configuration (AC: 1, 4)
  - [x] Reduce max_tokens from 4,000 to 3,200
  - [ ] Verify all 5 opportunity cards still generate
  - [ ] Check for truncated descriptions or actionability items
- [ ] Regression testing (AC: 3, 5)
  - [ ] Run 10 existing test documents with new token limits
  - [ ] Compare outputs with baseline (current token counts)
  - [ ] Verify no quality degradation
  - [ ] Check for any truncated content
- [ ] Performance validation (AC: 2)
  - [ ] Measure actual time savings (target: 10-15s)
  - [ ] Calculate cost savings per pipeline run
  - [ ] Project monthly savings based on expected usage
- [ ] Documentation updates
  - [ ] Update stage configuration documentation
  - [ ] Document token reduction rationale
  - [ ] Add monitoring alerts for future token adjustments

## Dev Notes

**Relevant Source Tree:**
- `backend/pipeline/stages/stage1_input_processing.py` - Stage 1 token config
- `backend/pipeline/stages/stage2_signal_amplification.py` - Stage 2 token config
- `backend/pipeline/stages/stage5_opportunity_generation.py` - Stage 5 token config
- `backend/pipeline/utils.py` - LLM invocation logic

**Token Reduction Plan:**

| Stage | Current | Target | Reduction | Rationale |
|-------|---------|--------|-----------|-----------|
| Stage 1 | 2,500 | 1,800 | -700 | Extractive task needs less generation |
| Stage 2 | 3,000 | 2,200 | -800 | Focused amplification is more concise |
| Stage 3 | 3,500 | 3,500 | 0 | Complex translation, keep unchanged |
| Stage 4 | 4,000 | 4,000 | 0 | Brand contextualization needs full tokens |
| Stage 5 | 4,000 | 3,200 | -800 | Structured JSON output is more compact |
| **Total** | **17,000** | **12,300** | **-4,700** | **28% reduction** |

**Key Implementation Notes:**
- Stages 3 and 4 unchanged (complex reasoning tasks)
- Stage 1 is extractive (less generation needed)
- Stage 2 amplification can be more concise
- Stage 5 uses structured JSON (inherently compact)
- Must test thoroughly to avoid truncation

**Important Constraints:**
- Cannot reduce Stage 5 below 3,200 tokens (5 opportunity cards require minimum space)
- Stage 3 (general translation) needs full token budget for quality
- Stage 4 (brand contextualization) is most critical for business value
- Must preserve existing prompt engineering logic

**Risk Mitigation:**
- Primary Risk: Token reduction causes truncated opportunity cards or insights
- Mitigation: A/B testing with current token counts, gradual reduction
- Rollback: Revert to original token counts in stage configuration files

**Code Changes Required:**
```python
# Stage 1: backend/pipeline/stages/stage1_input_processing.py
max_tokens=1800  # was 2500

# Stage 2: backend/pipeline/stages/stage2_signal_amplification.py
max_tokens=2200  # was 3000

# Stage 5: backend/pipeline/stages/stage5_opportunity_generation.py
max_tokens=3200  # was 4000
```

**Performance Expectations:**
- Current: 17,000 tokens total
- Target: 12,300 tokens total
- Time saved: 10-15 seconds (assuming ~300 tokens/second generation speed)
- Cost saved: 28% reduction in LLM API costs

### Testing

**Test File Location:**
- `backend/tests/test_token_optimization.py` (new file)
- `backend/tests/test_stage1_input_processing.py` (update)
- `backend/tests/test_stage2_signal_amplification.py` (update)
- `backend/tests/test_stage5_opportunity_generation.py` (update)

**Testing Standards:**
- Use pytest for regression testing
- Compare outputs with baseline test documents
- Manual quality review for truncation detection
- Automated JSON validation for Stage 5 completeness

**Testing Frameworks:**
- pytest for unit and integration tests
- JSON schema validation for Stage 5 outputs
- Diffing tools for output comparison

**Specific Testing Requirements:**
- **Regression Tests:** Run 10 existing test documents with new token limits
- **Output Comparison:** Side-by-side comparison of old vs new outputs
- **Truncation Detection:** Automated check for incomplete sentences or missing fields
- **Quality Review:** Business stakeholder approval on output quality
- **Performance Test:** Measure actual time savings with timing logs

**Test Documents to Use:**
- Same 10 documents used in current regression test suite
- Include edge cases: very long reports, very short reports, technical jargon-heavy

**Success Metrics:**
- All 10 test documents produce complete outputs
- No truncated opportunity cards
- No missing actionability items
- Quality scores remain within 5% of baseline

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-28 | 1.0 | Initial story creation from Epic 9 | John (PM) |

## Dev Agent Record

### Agent Model Used
claude-sonnet-4-5-20250929

### Debug Log References
N/A - straightforward token configuration updates

### Completion Notes List
- Token reductions implemented across 3 stages (Stage 1, 2, 5)
- Prompt updates added to emphasize conciseness
- Testing subtasks remain for validation

### File List
- backend/pipeline/stages/stage1_input_processing.py
- backend/pipeline/prompts/stage1_prompt.py
- backend/pipeline/stages/stage2_signal_amplification.py
- backend/pipeline/prompts/stage2_prompt.py
- backend/pipeline/stages/stage5_opportunity_generation.py

## QA Results
_To be populated by QA agent_
