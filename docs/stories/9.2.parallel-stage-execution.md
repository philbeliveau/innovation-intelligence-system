# Story 9.2: parallel-stage-execution

## Status
Draft

## Story
**As a** user analyzing innovation documents,
**I want** the pipeline to process my document as fast as possible,
**so that** I can iterate quickly and analyze multiple documents per session

## Acceptance Criteria

1. Pipeline completes in 60 seconds or less (down from 90s)
2. Stages 3 and 4 execute concurrently (currently sequential)
3. Error handling preserves individual stage failure detection
4. Database updates correctly reflect parallel execution state
5. No race conditions in opportunity card generation

## Tasks / Subtasks

- [ ] Refactor `execute_pipeline_background()` to use asyncio (AC: 2)
  - [ ] Convert Stage 3 execution to async function
  - [ ] Convert Stage 4 execution to async function
  - [ ] Implement `asyncio.gather()` for concurrent execution
- [ ] Add database transaction locks (AC: 4, 5)
  - [ ] Implement lock mechanism for opportunity card writes
  - [ ] Add stage state update synchronization
- [ ] Implement individual stage error handling (AC: 3)
  - [ ] Add try-catch per stage in parallel execution
  - [ ] Preserve stage failure logs independently
  - [ ] Ensure one stage failure doesn't crash the other
- [ ] Update pipeline state tracking (AC: 4)
  - [ ] Modify status endpoint to handle concurrent stage updates
  - [ ] Add timestamps for parallel stage start/complete
- [ ] Comprehensive testing (AC: 1-5)
  - [ ] Load testing with 10+ concurrent pipeline runs
  - [ ] Race condition testing for opportunity card generation
  - [ ] Error injection testing (Stage 3 fails, Stage 4 succeeds)
  - [ ] Verify 60-second completion time target

## Dev Notes

**Relevant Source Tree:**
- `backend/app/pipeline_runner.py` - Main pipeline execution logic to refactor
- `backend/pipeline/stages/stage3_general_translation.py` - Stage 3 implementation
- `backend/pipeline/stages/stage4_brand_contextualization.py` - Stage 4 implementation
- `backend/app/main.py` - Webhook endpoints for stage updates
- `innovation-web/app/api/pipeline/[runId]/status/route.ts` - Status polling endpoint

**Dependency Analysis:**
- Stage 1 → Stage 2 (must remain sequential)
- Stage 2 → Stage 3 (must remain sequential)
- **Stage 3 + Stage 4 can run in parallel** ⚡
- Stage 5 depends only on Stage 4 output (not Stage 3)

**Key Implementation Notes:**
- Use Python `asyncio` with `asyncio.gather(stage3_task, stage4_task)`
- Stage 3 and Stage 4 have no data dependencies on each other
- Both stages consume Stage 2 output independently
- Race condition risk exists in database writes during Stage 5
- Must preserve existing webhook notification pattern

**Important Constraints:**
- Railway deployment supports asyncio (no infrastructure changes needed)
- Prisma ORM used for database operations (supports async operations)
- Webhook notifications must still fire for each stage independently
- Frontend polling logic expects sequential stage numbers (no changes needed)

**Technical Implementation Details:**
```python
# Example asyncio pattern to implement
async def execute_stages_3_and_4_parallel(stage2_output):
    stage3_task = asyncio.create_task(execute_stage3(stage2_output))
    stage4_task = asyncio.create_task(execute_stage4(stage2_output))

    stage3_result, stage4_result = await asyncio.gather(
        stage3_task,
        stage4_task,
        return_exceptions=True
    )

    # Handle individual failures
    if isinstance(stage3_result, Exception):
        log_stage_failure(3, stage3_result)
    if isinstance(stage4_result, Exception):
        log_stage_failure(4, stage4_result)

    return stage3_result, stage4_result
```

**Risk Mitigation:**
- Primary Risk: Race conditions in database writes causing opportunity cards not to save
- Mitigation: Add database transaction locks using Prisma's transaction API
- Rollback: Feature flag to disable parallel execution and revert to sequential

### Testing

**Test File Location:**
- `backend/tests/test_parallel_execution.py` (new file)
- `backend/tests/test_pipeline_runner.py` (update existing)

**Testing Standards:**
- Use pytest with pytest-asyncio plugin
- Mock LLM API calls to control timing
- Use SQLite in-memory database for race condition testing
- Load testing requires Railway staging environment

**Testing Frameworks:**
- pytest for unit tests
- pytest-asyncio for async function testing
- locust or vegeta for load testing (staging only)

**Specific Testing Requirements:**
- **Unit Tests:** Verify asyncio.gather() correctly executes both stages
- **Integration Tests:** Test full pipeline with parallel execution
- **Concurrency Tests:** Run 10 pipelines simultaneously, verify no race conditions
- **Error Tests:** Inject failures in Stage 3 only, verify Stage 4 completes
- **Performance Tests:** Measure actual time savings (target: 30s reduction)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-28 | 1.0 | Initial story creation from Epic 9 | John (PM) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
