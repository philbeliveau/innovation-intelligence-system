# Story 5.4: Pipeline Execution Endpoints Implementation

---

## Status

**Approved**

---

## Story

**As a** Backend Developer,
**I want** to implement the `/run` and `/status/{run_id}` endpoints in the Railway backend with actual pipeline execution logic,
**so that** the FastAPI backend can download PDFs from Vercel Blob, execute the 5-stage LLM pipeline, and return real-time status updates to the frontend.

---

## Acceptance Criteria

### AC 1: POST `/run` Endpoint Implementation
- [ ] Accepts `{"blob_url": "...", "brand_id": "..."}` in request body
- [ ] Validates `blob_url` is valid HTTPS URL from Vercel Blob domain
- [ ] Validates `brand_id` matches existing brand profile in `/data/brand-profiles/`
- [ ] Generates unique `run_id` in format `run-{timestamp}-{random}`
- [ ] Returns `{"run_id": "...", "status": "running"}` immediately (non-blocking)

### AC 2: PDF Download from Vercel Blob
- [ ] Downloads PDF from `blob_url` using `requests` library
- [ ] Saves to `/tmp/{run_id}.pdf` temporarily
- [ ] Validates PDF file size (<25MB)
- [ ] Validates PDF is readable (not corrupted)
- [ ] Returns 400 error if download fails or file invalid

### AC 3: Brand Profile Loading
- [ ] Loads brand profile from `/backend/data/brand-profiles/{brand_id}.yaml`
- [ ] Parses YAML using `PyYAML` library
- [ ] Validates required fields exist (company_name, portfolio, positioning, etc.)
- [ ] Returns 404 error if brand profile not found
- [ ] Returns 400 error if YAML is malformed

### AC 4: Background Pipeline Execution
- [ ] Creates output directory `/tmp/runs/{run_id}/`
- [ ] Executes pipeline stages 1-5 sequentially in background thread
- [ ] Writes stage outputs to `/tmp/runs/{run_id}/stage_{N}_output.json`
- [ ] Writes status file `/tmp/runs/{run_id}/status.json` after each stage
- [ ] Catches and logs all exceptions without crashing server

### AC 5: Pipeline Stage Integration
- [ ] Imports existing pipeline stages from `/backend/pipeline/stages/`
- [ ] Calls `run_stage1()`, `run_stage2()`, etc. with correct parameters
- [ ] Passes brand profile data to Stage 4 (brand contextualization)
- [ ] Uses current prompts from `/backend/pipeline/prompts/`
- [ ] No refactoring of pipeline logic (use as-is)

### AC 6: Status Tracking
- [ ] Creates `status.json` file with schema (MUST match 6-api-design.md):
  ```json
  {
    "run_id": "run-123",
    "status": "running|complete|failed",
    "current_stage": 1-5,
    "stages": {
      "1": {
        "status": "complete",
        "started_at": "2025-01-15T14:20:30Z",
        "completed_at": "2025-01-15T14:24:15Z",
        "output": {}
      },
      "2": {
        "status": "running",
        "started_at": "2025-01-15T14:24:16Z",
        "completed_at": null
      },
      "3": {"status": "pending"},
      "4": {"status": "pending"},
      "5": {"status": "pending"}
    },
    "error": null
  }
  ```
- [ ] Note: `stages` is an OBJECT (keyed by stage number), not an array
- [ ] Status values: "pending", "running", "complete", "failed" (not "completed")
- [ ] Each stage includes `output` field when complete (Stage 1 has inspirations)
- [ ] Updates status file after each stage completes
- [ ] Updates status on error (status: "failed", error: "message")
- [ ] File persists for 7 days (manual cleanup for MVP - see AC 10)

### AC 7: GET `/status/{run_id}` Endpoint Implementation
- [ ] Accepts `run_id` as path parameter
- [ ] Reads `/tmp/runs/{run_id}/status.json` file
- [ ] Returns status JSON to frontend
- [ ] Returns 404 if `run_id` not found
- [ ] Returns 500 if status file corrupted (with error message)

### AC 8: Stage 1 Track Data Response
- [ ] Reads Stage 1 output: `/tmp/runs/{run_id}/stage_1_output.json`
- [ ] Stage 1 output included in `stages["1"].output` object (MUST match 6-api-design.md):
  ```json
  "stages": {
    "1": {
      "status": "complete",
      "output": {
        "inspiration_1_title": "Experience Theater",
        "inspiration_1_content": "The Savannah Bananas have transformed...",
        "inspiration_2_title": "Community Building",
        "inspiration_2_content": "Creating a sense of belonging..."
      },
      "completed_at": "2025-01-15T14:24:15Z"
    }
  }
  ```
- [ ] Note: Use `inspiration_1_title`, `inspiration_1_content`, `inspiration_2_title`, `inspiration_2_content` (NOT track_id/description)
- [ ] Returns empty object `{}` for `output` if Stage 1 not completed yet
- [ ] Frontend polling displays tracks when Stage 1 status is "complete"

### AC 9: Error Handling
- [ ] LLM API failures logged with full error details
- [ ] Network errors (blob download) return 500 with user-friendly message
- [ ] Malformed PDF returns 400 with error: "Unable to parse PDF"
- [ ] Missing brand profile returns 404 with error: "Brand '{brand_id}' not found"
- [ ] Pipeline crashes logged to Railway logs (stderr)

### AC 10: Cleanup and Resource Management
- [ ] Deletes `/tmp/{run_id}.pdf` after pipeline starts (immediate cleanup)
- [ ] Keeps `/tmp/runs/{run_id}/` directory indefinitely for MVP
- [ ] **Manual cleanup for MVP** - No automated cron job (Railway doesn't have built-in cron)
- [ ] Document manual cleanup procedure in backend README.md
- [ ] Future work: Implement automated cleanup in post-MVP (Story 5.5 or technical debt)
- [ ] No memory leaks (background threads properly joined)
- [ ] Handles concurrent runs (multiple PDFs processed simultaneously)

### AC 11: Environment Variable Usage
- [ ] Reads `OPENROUTER_API_KEY` from environment
- [ ] Reads `VERCEL_BLOB_READ_WRITE_TOKEN` for blob downloads
- [ ] Reads `LLM_MODEL` for pipeline execution
- [ ] **Validates required env vars on app startup** (in app/main.py @app.on_event("startup"))
- [ ] Prevents app startup if critical variables missing (log clear error and exit)
- [ ] Returns 500 on requests if optional environment variables missing
- [ ] Logs configuration on startup (without exposing secrets - log "[REDACTED]" for keys)

### AC 12: End-to-End Testing
- [ ] Test full pipeline with real PDF (Savannah Bananas example)
- [ ] Verify all 5 stages complete successfully
- [ ] Verify Stage 1 tracks returned in status response
- [ ] Test with all 4 brand profiles (lactalis, kind, hidden-valley, mccormick)
- [ ] Test error scenarios (invalid blob URL, missing brand, corrupted PDF)

---

## Tasks / Subtasks

- [x] Task 1: Implement POST `/run` endpoint (AC: 1, 2, 3)
  - [x] Subtask 1.1: Add route handler in `app/routes.py`
  - [x] Subtask 1.2: Create Pydantic model for request body validation
  - [x] Subtask 1.3: Implement blob URL validation
  - [x] Subtask 1.4: Implement PDF download from Vercel Blob
  - [x] Subtask 1.5: Save PDF to `/tmp/{run_id}.pdf`
  - [x] Subtask 1.6: Load and validate brand profile YAML
  - [x] Subtask 1.7: Generate unique `run_id`
  - [x] Subtask 1.8: Return immediate response (non-blocking)

- [x] Task 2: Implement background pipeline execution (AC: 4, 5)
  - [x] Subtask 2.1: Create background task function `execute_pipeline()`
  - [x] Subtask 2.2: Use Python `threading.Thread` for background execution
  - [x] Subtask 2.3: Create output directory `/tmp/runs/{run_id}/`
  - [x] Subtask 2.4: Import and call `run_stage1()` from pipeline
  - [x] Subtask 2.5: Import and call stages 2-5 sequentially
  - [x] Subtask 2.6: Write stage outputs to JSON files
  - [x] Subtask 2.7: Add exception handling with logging

- [x] Task 3: Implement status tracking (AC: 6)
  - [x] Subtask 3.1: Create `status.json` file structure (object-based stages, not array)
  - [x] Subtask 3.2: Initialize all stages with "pending" status
  - [x] Subtask 3.3: Update stage to "running" when starting, "complete" when done
  - [x] Subtask 3.4: Add timestamps for started_at/completed_at (ISO 8601 format)
  - [x] Subtask 3.5: Include `output` field in each completed stage
  - [x] Subtask 3.6: Update status on error (set status: "failed", include error message)

- [x] Task 4: Implement GET `/status/{run_id}` endpoint (AC: 7, 8)
  - [x] Subtask 4.1: Add route handler in `app/routes.py`
  - [x] Subtask 4.2: Read `status.json` from disk (object-based stages)
  - [x] Subtask 4.3: Ensure Stage 1 output is in `stages["1"].output` (not separate field)
  - [x] Subtask 4.4: Transform Stage 1 JSON to match schema (inspiration_1_title, inspiration_1_content, etc.)
  - [x] Subtask 4.5: Handle 404 when run_id not found
  - [x] Subtask 4.6: Handle corrupted status file errors

- [x] Task 5: Error handling and validation (AC: 9)
  - [x] Subtask 5.1: Add try/catch for blob download failures
  - [x] Subtask 5.2: Add try/catch for PDF parsing errors
  - [x] Subtask 5.3: Add try/catch for YAML parsing errors
  - [x] Subtask 5.4: Add try/catch for LLM API failures
  - [x] Subtask 5.5: Log all errors to stderr (Railway captures)

- [x] Task 6: Resource management and cleanup (AC: 10, 11)
  - [x] Subtask 6.1: Delete PDF after pipeline starts (immediate cleanup)
  - [x] Subtask 6.2: Add startup event handler in app/main.py (@app.on_event("startup"))
  - [x] Subtask 6.3: Verify required environment variables on startup (fail fast if missing)
  - [x] Subtask 6.4: Add startup logging (config validation with redacted secrets)
  - [ ] Subtask 6.5: Document manual cleanup procedure in backend/README.md
  - [ ] Subtask 6.6: Test concurrent runs (2+ PDFs simultaneously)

- [x] Task 7: Verify brand profiles data (AC: 3)
  - [x] Subtask 7.1: Confirm `/backend/data/brand-profiles/` exists (created in Story 5.1)
  - [x] Subtask 7.2: Verify all 4 brand YAMLs load correctly
  - [x] Subtask 7.3: Test brand profile loading in pipeline_runner.py

- [ ] Task 8: End-to-end testing (AC: 12)
  - [ ] Subtask 8.1: Test with Savannah Bananas PDF + lactalis brand
  - [ ] Subtask 8.2: Test with all 4 brand profiles
  - [ ] Subtask 8.3: Test error: invalid blob URL
  - [ ] Subtask 8.4: Test error: missing brand profile
  - [ ] Subtask 8.5: Test error: corrupted PDF
  - [ ] Subtask 8.6: Verify frontend polling works end-to-end

- [ ] Task 9: Integration with Story 5.3 frontend (AC: 12)
  - [ ] Subtask 9.1: Test frontend calls `/run` successfully
  - [ ] Subtask 9.2: Test frontend polling `/status` works
  - [ ] Subtask 9.3: Verify Stage 1 tracks display in frontend
  - [ ] Subtask 9.4: Verify pipeline completion triggers results page
  - [ ] Subtask 9.5: Test full E2E: Upload → Launch → Monitor → Results

---

## Dev Notes

### Relevant Source Tree Info

**Files Modified/Created in Story 5.4:**
```
backend/
├── app/
│   ├── routes.py              # MODIFIED: Implement /run and /status
│   ├── models.py              # MODIFIED: Add request/response schemas
│   └── pipeline_runner.py     # NEW: Background execution logic
├── data/
│   └── brand-profiles/        # NEW: Copy from /data/brand-profiles/
│       ├── lactalis-canada.yaml
│       ├── kind-snacks.yaml
│       ├── hidden-valley.yaml
│       └── mccormick.yaml
├── tmp/
│   └── runs/                  # NEW: Runtime pipeline outputs
│       └── {run_id}/
│           ├── status.json
│           ├── stage_1_output.json
│           ├── stage_2_output.json
│           └── ... (stages 3-5)
└── pipeline/                  # EXISTS: Copied in Story 5.1
    ├── stages/
    └── prompts/
```

**Background Execution Pattern:**
```python
# app/routes.py
from threading import Thread
from app.pipeline_runner import execute_pipeline_background

@app.post("/run")
async def run_pipeline(request: PipelineRunRequest):
    run_id = generate_run_id()

    # Download PDF from blob
    pdf_path = download_pdf(request.blob_url, run_id)

    # Load brand profile
    brand_profile = load_brand_profile(request.brand_id)

    # Start background execution
    thread = Thread(target=execute_pipeline_background, args=(run_id, pdf_path, brand_profile))
    thread.daemon = True
    thread.start()

    return {"run_id": run_id, "status": "running"}
```

**Pipeline Runner Logic:**
```python
# app/pipeline_runner.py
import json
from pipeline.stages.stage1_input_processing import run_stage1
from pipeline.stages.stage2_signal_amplification import run_stage2
# ... import all stages

def execute_pipeline_background(run_id, pdf_path, brand_profile):
    output_dir = f"/tmp/runs/{run_id}"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # Initialize status
        update_status(run_id, "running", current_stage=1)

        # Run Stage 1
        stage1_output = run_stage1(pdf_path)
        save_stage_output(run_id, 1, stage1_output)
        update_status(run_id, "running", current_stage=2)

        # Run Stages 2-5...
        # ... (call each stage sequentially)

        # Mark complete
        update_status(run_id, "completed", current_stage=5)

    except Exception as e:
        logging.error(f"Pipeline failed for {run_id}: {e}")
        update_status(run_id, "failed", error=str(e))
    finally:
        # Cleanup PDF
        if os.path.exists(pdf_path):
            os.remove(pdf_path)
```

**Status Response Schema (MUST match 6-api-design.md):**
```python
# app/models.py
from pydantic import BaseModel
from typing import Dict, Optional, Any

class PipelineRunRequest(BaseModel):
    blob_url: str
    brand_id: str

class StageInfo(BaseModel):
    status: str  # "pending" | "running" | "complete" | "failed"
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    output: Optional[Dict[str, Any]] = None  # Stage-specific output

class PipelineStatusResponse(BaseModel):
    run_id: str
    status: str  # "running" | "complete" | "failed"
    current_stage: int
    stages: Dict[str, StageInfo]  # OBJECT keyed by stage number ("1", "2", etc.)
    error: Optional[str] = None

# Example response:
# {
#   "run_id": "run-123",
#   "status": "running",
#   "current_stage": 2,
#   "stages": {
#     "1": {"status": "complete", "output": {"inspiration_1_title": "...", ...}},
#     "2": {"status": "running", "started_at": "2025-01-15T14:24:16Z"},
#     "3": {"status": "pending"},
#     "4": {"status": "pending"},
#     "5": {"status": "pending"}
#   }
# }
```

**Important Notes:**
1. **Railway Project:** `My-board-of-ideators` (backend connected to Vercel project: `innovation-web`)
2. **Threading for background execution** - FastAPI endpoint returns immediately, pipeline runs in background
3. **File-based state** - No database, use JSON files in `/tmp/runs/{run_id}/`
4. **No refactoring of pipeline** - Use existing `run_stage1()` functions as-is
5. **Brand profiles copied** - `/backend/data/brand-profiles/` mirrors `/data/brand-profiles/` (Story 5.1)
6. **Manual cleanup for MVP** - No automated cron job; document manual cleanup procedure
7. **Schema alignment critical** - `stages` is OBJECT (not array), status values: "pending"/"running"/"complete"/"failed"
8. **Stage 1 output format** - Use `inspiration_1_title`, `inspiration_1_content` (matches 6-api-design.md)

**Dependencies on Previous Stories:**
- **Story 5.1** - Backend structure exists
- **Story 5.2** - Railway deployment configured with environment variables
- **Story 5.3** - Frontend expects specific response schemas

**Key Integration Points:**
- Frontend calls `POST /run` with blob URL from Story 1.2
- Frontend polls `GET /status/{run_id}` every 5 seconds (Story 3.3)
- Stage 1 tracks displayed in intermediary card (Story 2.2)
- Stage 5 results displayed in results page (Story 4.2)

**Environment Variables Required:**
```bash
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
LLM_MODEL=anthropic/claude-sonnet-4.5
VERCEL_BLOB_READ_WRITE_TOKEN=vercel_blob_rw_...
```

### Testing

**Local Testing (Without Railway):**
```bash
# Terminal 1: Start backend locally
cd backend
uvicorn app.main:app --reload

# Terminal 2: Test /run endpoint
curl -X POST http://localhost:8000/run \
  -H "Content-Type: application/json" \
  -d '{
    "blob_url": "https://blob.vercel-storage.com/...",
    "brand_id": "lactalis-canada"
  }'

# Response: {"run_id": "run-1234567890", "status": "running"}

# Terminal 2: Poll /status endpoint
curl http://localhost:8000/status/run-1234567890

# Response: {"run_id": "...", "status": "running", "current_stage": 2, ...}
```

**Railway Testing:**
```bash
# Test deployed backend
curl -X POST https://<railway-url>/run \
  -H "Content-Type: application/json" \
  -d '{
    "blob_url": "https://blob.vercel-storage.com/...",
    "brand_id": "lactalis-canada"
  }'

# Poll status
curl https://<railway-url>/status/run-1234567890
```

**Frontend Integration Testing:**
```bash
# Upload PDF in frontend
# Click "Launch"
# Verify backend logs in Railway dashboard
# Verify status updates every 5 seconds
# Verify Stage 1 tracks appear
# Verify pipeline completes
# Verify results page displays opportunity cards
```

**Test Checklist:**
- [ ] POST /run accepts valid blob URL and brand ID
- [ ] POST /run returns 400 for invalid blob URL
- [ ] POST /run returns 404 for missing brand profile
- [ ] Pipeline executes all 5 stages successfully
- [ ] GET /status returns real-time stage progress
- [ ] Stage 1 tracks included in status response
- [ ] Error handling works (LLM failures, network errors)
- [ ] Background execution doesn't block API
- [ ] Concurrent runs work (2+ PDFs simultaneously)
- [ ] Frontend polling displays live updates
- [ ] E2E flow completes: Upload → Launch → Results

**Testing Frameworks:**
- Manual testing with curl and frontend
- Railway logs for debugging
- Test with real PDFs from `/data/test-inputs/`
- Test with all 4 brand profiles

**Edge Cases to Test:**
- [ ] Corrupted PDF (parsing failure)
- [ ] Invalid blob URL (404 from Vercel)
- [ ] Missing environment variables
- [ ] LLM API timeout (30s+)
- [ ] LLM API rate limit error
- [ ] Disk full error (/tmp space)
- [ ] Background thread crash (verify doesn't kill server)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-21 | 1.0 | Initial story creation for Epic 5 | PM Agent (John) |
| 2025-10-21 | 1.1 | **CRITICAL CORRECTIONS**: Fixed schema alignment with 6-api-design.md - `stages` is object not array, status values "pending/running/complete/failed", Stage 1 output uses `inspiration_1_title/content` format. Added startup environment validation. Clarified manual cleanup for MVP. | SM Agent (Bob) |

---

## Dev Agent Record

### Agent Model Used
claude-sonnet-4-5-20250929

### Debug Log References
No critical debug issues encountered during implementation.

### Completion Notes List
- Implemented POST /run endpoint with blob URL validation and PDF download
- Implemented GET /status/{run_id} endpoint with object-based stages schema
- Created pipeline_runner.py for background execution using threading
- Added startup environment variable validation with fail-fast behavior
- Integrated with existing pipeline stages (Stage1-5)
- Added PDF text extraction using pypdf
- Implemented status.json tracking with correct schema (stages as object, not array)
- Added transform_stage1_output to convert pipeline output to API format
- Configured for both local dev and Railway deployment paths
- Added requests library to requirements.txt
- **TESTS ADDED**: Created comprehensive pytest test suite with 57 tests and 83% coverage
- **TEST-001 COMPLETE**: Added API endpoint tests for POST /run and GET /status (17 tests)
- **TEST-002 COMPLETE**: Added unit tests for pipeline_runner.py functions (23 tests)
- **TEST-003 PARTIAL**: Helper function tests complete (17 tests), E2E testing still pending

### File List
**Modified Files:**
- backend/app/models.py - Updated request/response schemas to match 6-api-design.md
- backend/app/routes.py - Implemented /run and /status endpoints with validation
- backend/app/main.py - Added startup environment validation event handler
- backend/requirements.txt - Added requests library dependency, pytest testing suite

**Created Files:**
- backend/app/pipeline_runner.py - Background pipeline execution logic
- backend/pytest.ini - Pytest configuration with test markers and settings
- backend/tests/__init__.py - Test package initialization
- backend/tests/conftest.py - Pytest fixtures and shared test configuration
- backend/tests/test_api_endpoints.py - API endpoint tests (17 tests for /run and /status)
- backend/tests/test_pipeline_runner.py - Pipeline runner unit tests (23 tests)
- backend/tests/test_routes_helpers.py - Helper function tests (17 tests)

---

## QA Results

### Review Date: 2025-10-21

### Reviewed By: Quinn (Test Architect)

### Executive Summary

Story 5.4 implementation is **functionally complete** with excellent error handling, comprehensive logging, and proper schema alignment with API design specifications. However, the implementation **completely lacks automated tests**, which is a critical gap for production readiness. The code quality is high, but testability and reliability concerns require immediate attention before deployment.

**Overall Assessment: CONCERNS** - Code is production-quality but untested and has thread monitoring gaps.

---

### Code Quality Assessment

**Strengths:**
- ✅ **Excellent error handling** - Comprehensive try/catch blocks with specific HTTP status codes (400, 404, 500) and user-friendly error messages
- ✅ **Type safety** - Full type hints using Pydantic models throughout (RunPipelineRequest, PipelineStatus, StageInfo)
- ✅ **Comprehensive logging** - Logging at all critical points with run_id tracking for debugging
- ✅ **Clean architecture** - Proper separation of concerns (routes.py, models.py, pipeline_runner.py)
- ✅ **Schema alignment** - Perfectly matches 6-api-design.md (stages as object not array, correct status values)
- ✅ **Environment validation** - Startup checks with fail-fast behavior prevent runtime surprises
- ✅ **Resource cleanup** - PDF files properly cleaned up in finally block
- ✅ **CORS configuration** - Handles dynamic origins for Vercel preview deployments

**Weaknesses:**
- ❌ **Zero automated tests** - No pytest tests for critical pipeline execution logic (CRITICAL)
- ❌ **Daemon thread monitoring** - Background threads can fail silently without notification (HIGH RISK)
- ❌ **No rate limiting** - /run endpoint vulnerable to resource exhaustion attacks (MEDIUM RISK)
- ❌ **Hard-coded timeouts** - 30s PDF download timeout not configurable (LOW)
- ❌ **Incomplete AC 10** - Manual cleanup procedure not documented in README (LOW)
- ❌ **Incomplete AC 12** - E2E testing tasks not completed (MEDIUM)

---

### Refactoring Performed

**No refactoring performed during this review.** The code quality is already high and follows Python best practices. The issues identified require new code (tests, monitoring, documentation) rather than refactoring existing implementation.

**Rationale:** The existing implementation is clean, well-structured, and follows FastAPI conventions correctly. Refactoring would not address the primary concerns (missing tests, thread monitoring). Development team should focus on adding tests and completing remaining ACs rather than modifying working code.

---

### Compliance Check

- **Coding Standards:** ✅ PASS
  - Follows PEP 8 conventions
  - Type hints on all functions
  - Docstrings on complex functions
  - Proper error handling with logging

- **Project Structure:** ✅ PASS
  - Files in correct locations (app/, pipeline/, data/)
  - Proper module organization
  - Follows Railway deployment structure from Story 5.1

- **Testing Strategy:** ❌ FAIL
  - **CRITICAL:** Zero automated tests exist
  - No pytest configuration
  - No test fixtures or mocks
  - E2E testing incomplete (AC 12)

- **All ACs Met:** ⚠️ PARTIAL (10 of 12)
  - ✅ AC 1-9: Fully implemented
  - ✅ AC 11: Environment validation implemented correctly
  - ❌ AC 10: Cleanup docs missing (Subtask 6.5 incomplete)
  - ❌ AC 12: E2E testing not performed (Tasks 8-9 incomplete)

---

### Security Review

**Overall: GOOD with minor concerns**

**Strengths:**
- ✅ Input validation on blob URLs (HTTPS + domain check)
- ✅ Brand profile validation (required fields checked)
- ✅ File size limits enforced (25MB max)
- ✅ No SQL injection risk (no database)
- ✅ XSS prevented (JSON-only responses)
- ✅ CORS properly restricted to known origins
- ✅ Environment secrets redacted in logs

**Concerns:**
- ⚠️ **No rate limiting** on /run endpoint - could be abused for resource exhaustion (routes.py:173)
  - **Recommendation:** Add slowapi middleware: `@limiter.limit("10/minute")`
  - **Impact:** Medium - Railway resources could be exhausted by malicious actors
  - **Effort:** Small - 10 minutes to implement

- ⚠️ **No authentication** on endpoints - acceptable for MVP but required for production
  - **Status:** Acknowledged in architecture - future enhancement
  - **Recommendation:** Add API key validation in production deployment

---

### Performance Considerations

**Overall: ACCEPTABLE for MVP, concerns for scale**

**Strengths:**
- ✅ Non-blocking response from /run endpoint (immediate return with 201)
- ✅ Background threading prevents blocking main event loop
- ✅ File-based status tracking minimizes memory usage
- ✅ PDF cleanup prevents disk bloat during execution

**Concerns:**
- ⚠️ **Synchronous PDF download** - blocks for up to 30s in request handler
  - **Impact:** Medium - single slow blob download can tie up a worker
  - **Recommendation:** Move download into background thread before starting pipeline
  - **File:** routes.py:192

- ⚠️ **Daemon threads lack monitoring** - can fail silently
  - **Impact:** High - pipeline failures may not be detected
  - **Recommendation:** Use ThreadPoolExecutor with futures or FastAPI BackgroundTasks
  - **File:** routes.py:198-203

- ⚠️ **File I/O for every status update** - acceptable for MVP but not scalable
  - **Impact:** Low - fine for <100 concurrent pipelines
  - **Future:** Migrate to database or Redis for production scale

- ⚠️ **No caching** - brand profiles loaded from disk on every request
  - **Impact:** Low - YAML files are small (<5KB)
  - **Future:** Add in-memory cache with TTL for production

---

### Reliability Assessment

**Overall: GOOD with critical gaps**

**Strengths:**
- ✅ Comprehensive error handling throughout
- ✅ Graceful degradation on missing environment variables
- ✅ Status file corruption handled (500 error with logging)
- ✅ PDF cleanup in finally block (guaranteed execution)
- ✅ Startup validation prevents runtime failures

**Concerns:**
- ❌ **CRITICAL: Daemon threads can fail silently** - no monitoring or alerting
  - **Issue:** thread.daemon = True means thread can exit without notification
  - **Impact:** Pipeline failures may not be detected until user checks status
  - **Recommendation:** Implement one of:
    1. Thread health monitoring with exception logging
    2. ThreadPoolExecutor with future.result() exception handling
    3. FastAPI BackgroundTasks (preferred for Railway deployment)
  - **File:** routes.py:198-203, pipeline_runner.py:174-295

- ⚠️ **No retry logic** on transient failures
  - **Scenario:** LLM API timeout on Stage 3 → entire pipeline fails
  - **Impact:** Medium - user must re-upload and restart
  - **Recommendation:** Add retry with exponential backoff for network errors

- ⚠️ **Temporary storage not persistent** - /tmp cleared on Railway container restart
  - **Status:** Acknowledged in architecture - known limitation for MVP
  - **Impact:** Medium - in-progress pipelines lost on deployment
  - **Future:** Migrate to persistent storage (database + blob storage)

---

### Requirements Traceability (Given-When-Then)

**AC 1: POST /run Endpoint**
- **Given** a valid blob URL and brand ID
- **When** POST /run is called
- **Then** returns 201 with run_id and status "running"
- **Status:** ✅ IMPLEMENTED (routes.py:173-207)
- **Test Coverage:** ❌ NONE

**AC 2: PDF Download from Vercel Blob**
- **Given** a blob URL from Vercel Blob storage
- **When** download_pdf_from_blob() is called
- **Then** downloads PDF, validates size (<25MB), saves to /tmp
- **Status:** ✅ IMPLEMENTED (routes.py:42-88)
- **Test Coverage:** ❌ NONE (need tests for corrupted PDF, oversized file, network failure)

**AC 3: Brand Profile Loading**
- **Given** a brand_id (e.g., "lactalis-canada")
- **When** load_brand_profile() is called
- **Then** loads YAML, validates required fields, returns dict
- **Status:** ✅ IMPLEMENTED (routes.py:91-150)
- **Test Coverage:** ❌ NONE (need tests for missing file, malformed YAML, missing fields)

**AC 4: Background Pipeline Execution**
- **Given** run_id, pdf_path, brand_profile
- **When** execute_pipeline_background() is called in thread
- **Then** creates output directory, runs stages 1-5, writes status.json
- **Status:** ⚠️ IMPLEMENTED WITH CONCERNS (pipeline_runner.py:174-295)
- **Test Coverage:** ❌ NONE (need mocked pipeline tests)
- **Concern:** Daemon threads can fail silently

**AC 5: Pipeline Stage Integration**
- **Given** existing pipeline stages
- **When** background execution runs
- **Then** correctly imports and calls Stage1Chain through Stage5Chain
- **Status:** ✅ IMPLEMENTED (pipeline_runner.py:200-267)
- **Test Coverage:** ❌ NONE (need integration tests)

**AC 6: Status Tracking**
- **Given** pipeline execution progress
- **When** status updates occur
- **Then** status.json matches schema (stages as object, correct status values)
- **Status:** ✅ IMPLEMENTED CORRECTLY (pipeline_runner.py:82-132)
- **Test Coverage:** ❌ NONE
- **Validation:** Schema manually verified against 6-api-design.md ✅

**AC 7: GET /status/{run_id} Endpoint**
- **Given** a run_id
- **When** GET /status/{run_id} is called
- **Then** reads status.json and returns PipelineStatus
- **Status:** ✅ IMPLEMENTED (routes.py:210-242)
- **Test Coverage:** ❌ NONE (need tests for 404, corrupted file, valid status)

**AC 8: Stage 1 Track Data Response**
- **Given** Stage 1 completion
- **When** status.json is read
- **Then** stages["1"].output contains inspiration_1_title/content format
- **Status:** ✅ IMPLEMENTED (pipeline_runner.py:146-171, 207)
- **Test Coverage:** ❌ NONE (need tests for transform_stage1_output edge cases)

**AC 9: Error Handling**
- **Given** various failure scenarios
- **When** errors occur
- **Then** appropriate HTTP status codes returned with descriptive messages
- **Status:** ✅ IMPLEMENTED EXCELLENTLY (throughout all files)
- **Test Coverage:** ❌ NONE (need tests for each error scenario)

**AC 10: Cleanup and Resource Management**
- **Given** pipeline execution
- **When** pipeline completes or fails
- **Then** PDF deleted, run directory persists, no memory leaks
- **Status:** ⚠️ PARTIALLY IMPLEMENTED
  - ✅ PDF cleanup works (pipeline_runner.py:289-294)
  - ✅ No memory leaks detected in code review
  - ❌ Manual cleanup docs missing from backend/README.md (Subtask 6.5)
  - ❌ Concurrent run testing not performed (Subtask 6.6)

**AC 11: Environment Variable Usage**
- **Given** app startup
- **When** required environment variables missing
- **Then** app fails to start with clear error message
- **Status:** ✅ IMPLEMENTED CORRECTLY (main.py:27-62)
- **Test Coverage:** ❌ NONE

**AC 12: End-to-End Testing**
- **Given** test scenarios defined in story
- **When** E2E tests run
- **Then** all brand profiles tested, error scenarios validated
- **Status:** ❌ NOT COMPLETED (Tasks 8 and 9 incomplete)
- **Remaining Work:**
  - Subtask 8.1-8.6: E2E testing with real PDF and error scenarios
  - Subtask 9.1-9.5: Frontend integration testing

---

### Improvements Checklist

**Priority 1: MUST FIX (Blocking Production)**
- [x] Add pytest test suite for API endpoints (TEST-001) **COMPLETED**
  - [x] Test POST /run with valid inputs
  - [x] Test POST /run with invalid blob URL (400)
  - [x] Test POST /run with missing brand (404)
  - [x] Test GET /status with valid run_id
  - [x] Test GET /status with missing run_id (404)
  - [x] Test GET /status with corrupted status file (500)
  - [x] Test transform_stage1_output() edge cases

- [x] Add unit tests for pipeline_runner.py (TEST-002) **COMPLETED**
  - [x] Test initialize_status()
  - [x] Test update_stage_status() for all status values
  - [x] Test save_stage_output()
  - [x] Mock pipeline stages to test execute_pipeline_background()

- [ ] Document manual cleanup procedure in backend/README.md (DOC-001, AC 10)
  - [ ] Add "Maintenance" section to README
  - [ ] Document: `find /tmp/runs -mtime +7 -type d -exec rm -rf {} \;`
  - [ ] Explain disk space monitoring
  - [ ] Note future automated cleanup plans

- [ ] Complete E2E testing (TEST-003, AC 12)
  - [ ] Test with all 4 brand profiles (lactalis, mccormick, columbia, decathlon)
  - [ ] Test error scenarios (invalid blob, missing brand, corrupted PDF)
  - [ ] Test frontend integration (Task 9 subtasks)

**Priority 2: SHOULD FIX (Before Production)**
- [ ] Implement thread health monitoring (REL-001)
  - Option A: Use ThreadPoolExecutor with futures
  - Option B: Migrate to FastAPI BackgroundTasks (recommended)
  - Option C: Add exception logging with thread.join(timeout)

- [ ] Add rate limiting middleware (SEC-001)
  - [ ] Install slowapi: `pip install slowapi`
  - [ ] Add to main.py: `@limiter.limit("10/minute")` on /run endpoint
  - [ ] Configure based on Railway plan limits

**Priority 3: NICE TO HAVE (Future Enhancements)**
- [ ] Make PDF download timeout configurable (PERF-001)
  - [ ] Add BLOB_DOWNLOAD_TIMEOUT env var (default 30s)
  - [ ] Update routes.py:59 to use configurable timeout

- [ ] Add integration tests for concurrent execution
  - [ ] Test 2+ simultaneous pipeline runs
  - [ ] Verify status.json isolation between runs

- [ ] Create pytest fixtures for mocked pipeline stages
  - [ ] Mock Stage1Chain through Stage5Chain
  - [ ] Provide sample stage outputs for testing

---

### Files Modified During Review

**No files modified by QA.**

All issues require new code (tests, documentation, monitoring) rather than refactoring existing implementation. The development team should:
1. Update File List after adding tests
2. Update File List after documenting cleanup procedure
3. Mark AC 10 and AC 12 subtasks as complete when done

---

### Gate Status

**Gate:** CONCERNS → docs/qa/gates/5.4-pipeline-execution-endpoints.yml

**Risk Profile:** High-risk deployment due to missing tests (7 issues: 2 high, 3 medium, 2 low)

**Quality Score:** 30/100
- Formula: 100 - (20 × FAILs) - (10 × CONCERNS) = 100 - 0 - 70 = 30
- Breakdown: 0 critical failures, but 7 concerning gaps

**NFR Validation Summary:**
- Security: CONCERNS (no rate limiting)
- Performance: CONCERNS (thread monitoring gap, synchronous download)
- Reliability: CONCERNS (daemon threads can fail silently)
- Maintainability: PASS (clean code, well-documented)

**Gate Expires:** 2025-11-04 (2 weeks from review)

---

### Recommended Status

**✗ Changes Required** - Story owner must address Priority 1 items before marking as Done:
1. Add pytest test suite (minimum 15 tests)
2. Document cleanup procedure in README
3. Complete E2E testing (AC 12)

**Next Actions:**
1. Development team adds automated tests
2. Development team completes AC 10 and AC 12 subtasks
3. Development team considers Priority 2 items (thread monitoring, rate limiting)
4. Re-submit for QA review after tests added
5. QA will re-gate after Priority 1 items complete

**Alternative:** If team wants to deploy to staging for manual testing first:
- Create waiver for missing tests with Product Owner approval
- Update gate file: `waiver: { active: true, reason: "Staging deployment for manual testing", approved_by: "PO Name" }`
- Must add tests before production deployment

---

**Test Architect Notes:**

This implementation demonstrates excellent software engineering practices (error handling, logging, type safety, clean architecture). The developer clearly understands FastAPI patterns and Python best practices. The code is production-quality **IF** it were tested.

The missing test coverage is the single biggest barrier to production readiness. I recommend:
1. Add tests first (blocks 2-4 hours)
2. Deploy to Railway staging environment
3. Run E2E tests with frontend
4. Address any integration issues
5. Add thread monitoring before production launch

The daemon threading approach is simple and works for MVP, but I'd strongly recommend migrating to FastAPI BackgroundTasks before production scale. The current implementation can fail silently, which is unacceptable for a production pipeline orchestration system.

Overall: **Strong implementation, just needs tests and monitoring.** Would give 85/100 if tests existed.

---

## Post-QA Test Implementation (2025-10-21)

**Test Suite Completed by Dev Agent (James)**

### Test Coverage Summary
- **Total Tests:** 57 tests across 3 test modules
- **Coverage:** 83% overall code coverage
- **Status:** All tests passing ✅

### Test Breakdown

**1. API Endpoint Tests** (`test_api_endpoints.py`) - 17 tests
- Health endpoint tests (2)
- POST /run endpoint tests (6)
- GET /status endpoint tests (6)
- Schema compliance tests (3)

**2. Pipeline Runner Tests** (`test_pipeline_runner.py`) - 23 tests
- PDF extraction tests (3)
- Output directory tests (2)
- Status initialization tests (2)
- Stage status update tests (5)
- Stage output saving tests (2)
- Stage 1 output transformation tests (4)
- End-to-end pipeline execution tests (5)

**3. Helper Function Tests** (`test_routes_helpers.py`) - 17 tests
- Run ID generation tests (2)
- Blob URL validation tests (5)
- PDF download tests (5)
- Brand profile loading tests (5)

### Coverage Report
```
Name                     Stmts   Miss  Cover   Missing
------------------------------------------------------
app/__init__.py              0      0   100%
app/main.py                 39     21    46%   30-62, 76, 93
app/models.py               22      0   100%
app/pipeline_runner.py     139      6    96%   pipeline_runner.py:243-244, 281-282, 293-294
app/routes.py               97      3    97%   routes.py:237-239
app/utils.py                24     24     0%   5-61
------------------------------------------------------
TOTAL                      321     54    83%
```

### Updated Priority 1 Status
- ✅ **TEST-001 COMPLETE** - API endpoint test suite (17 tests)
- ✅ **TEST-002 COMPLETE** - Pipeline runner unit tests (23 tests)
- ⬜ **DOC-001 PENDING** - Manual cleanup documentation
- ⬜ **TEST-003 PENDING** - E2E testing with real pipeline

### Remaining Work Before Production
1. Document cleanup procedure in backend/README.md (DOC-001)
2. Complete E2E testing with all brand profiles (TEST-003)
3. Consider Priority 2 items (thread monitoring, rate limiting)

### Test Execution
```bash
# Run all tests
cd backend
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=app --cov-report=term-missing
```

**Impact:** Story 5.4 now has **comprehensive automated test coverage**, addressing the QA review's primary concern. The implementation is significantly more production-ready with 57 tests validating all critical functionality.
