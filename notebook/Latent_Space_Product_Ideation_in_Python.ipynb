{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŒŒ Latent Space Ideation with Advanced Swarm Queen Optimization\n",
        "\n",
        "This Google Colab notebook implements a powerful **latent space ideation framework** that combines the strengths of modern language models and swarm intelligence. It allows users to **generate novel, high-quality ideas** by navigating and manipulating embedding spaces using a custom-built Swarm Queen Optimization (SQO) mechanism.\n",
        "\n",
        "### ðŸ” What This Notebook Does\n",
        "\n",
        "* **Encodes ideas into embeddings** using `sentence-transformers`\n",
        "* **Explores latent space** via a swarm of intelligent agents guided by a central \"Swarm Queen\"\n",
        "* **Interpolates and mutates concepts** in vector space to spark new innovations\n",
        "* **Decodes ideas back to human language** using a large generative language model (e.g., `google/gemma-2b-it`)\n",
        "* **Scores and ranks ideas** with a placeholder evaluator (easily replaceable with an LLM-based judge)\n",
        "\n",
        "### ðŸ§  Key Components\n",
        "\n",
        "* `TextEncoder`: Encodes text into latent space embeddings\n",
        "* `SwarmAgent`: Generates novel idea vectors using guided interpolation and noise\n",
        "* `SwarmQueen`: Maintains global guidance vector and adapts based on feedback\n",
        "* `Projector`: Neural mapper to align encoder and decoder vector dimensions\n",
        "* `LatentToTextDecoder`: Generates human-readable text from latent vectors\n",
        "* `run_latent_sqo_pipeline()`: Orchestrates the entire pipeline\n",
        "\n",
        "### ðŸ§ª Real-World Use Case\n",
        "\n",
        "Combines:\n",
        "\n",
        "> ðŸ§  *\"A smart water bottle that tracks hydration\"*\n",
        "> with\n",
        "> â˜€ï¸ *\"A solar-powered portable charger\"*\n",
        "\n",
        "To discover:\n",
        "\n",
        "> ðŸ’¡ *\"A solar-powered smart water bottle that tracks hydration and purifies water using sunlight.\"*\n",
        "\n",
        "### âš™ï¸ Runtime Instructions\n",
        "\n",
        "1. **Change Runtime Type:**\n",
        "   In the Colab menu, go to **`Runtime` â†’ `Change runtime type`** and select **GPU (T4 or A100 preferred)**.\n",
        "\n",
        "2. **Run Cells in Order:**\n",
        "   Start from installing dependencies, then run each component step-by-step.\n",
        "\n",
        "3. **Set Your HuggingFace Token (Optional):**\n",
        "   If loading private models, securely pass your token using:\n",
        "\n",
        "   ```python\n",
        "   from google.colab import userdata  \n",
        "   HF_TOKEN = userdata.get('Hugging')  \n",
        "   ```\n",
        "\n",
        "### ðŸ§© Plug-and-Play:\n",
        "\n",
        "* Easily swap out encoder or decoder models (e.g., OpenRouter or local LLaMA variants).\n",
        "* Add a real evaluator by integrating GPT-4 or Claude 3.5 with scoring prompts.\n",
        "\n"
      ],
      "metadata": {
        "id": "alO8amVWuslB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve your secret API key\n",
        "HF_TOKEN = userdata.get('Hugging')\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "7__aCkPfkYHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "# pip install sentence-transformers numpy\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "class LatentSpaceIdeator:\n",
        "    \"\"\"\n",
        "    A simple implementation of the latent space ideation framework.\n",
        "    This class can encode text ideas into embeddings, combine them,\n",
        "    and then would typically use a decoder to generate a new idea.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initializes the ideator with a sentence-transformer model.\n",
        "\n",
        "        Args:\n",
        "            encoder_model_name (str): The name of the pre-trained model to use for encoding.\n",
        "        \"\"\"\n",
        "        print(\"Loading the encoder model... This may take a moment.\")\n",
        "        # The encoder model turns text into meaningful numerical vectors (embeddings)\n",
        "        self.encoder = SentenceTransformer(encoder_model_name)\n",
        "        print(\"Encoder model loaded successfully.\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a single piece of text into a latent space vector.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to encode.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: A numpy array representing the text in the latent space.\n",
        "        \"\"\"\n",
        "        # The .encode() method creates the vector representation of the text.\n",
        "        return self.encoder.encode(text)\n",
        "\n",
        "    def interpolate(self, vec1, vec2, weight=0.5):\n",
        "        \"\"\"\n",
        "        Creates a new vector by interpolating between two existing vectors.\n",
        "        This is the core of the 'combining' process in the latent space.\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.ndarray): The first vector.\n",
        "            vec2 (np.ndarray): The second vector.\n",
        "            weight (float): The weight for interpolation. 0.5 gives an even mix.\n",
        "                            Closer to 0 leans towards vec1, closer to 1 leans towards vec2.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The new, interpolated vector.\n",
        "        \"\"\"\n",
        "        # Linear interpolation: new_vector = (1 - weight) * vec1 + weight * vec2\n",
        "        return (1 - weight) * vec1 + weight * vec2\n",
        "\n",
        "    def find_most_similar(self, target_vector, candidate_texts):\n",
        "        \"\"\"\n",
        "        A simple 'decoder' that finds the most similar text to a target vector.\n",
        "        In a full implementation (like the paper), this would be a generative\n",
        "        language model that generates text *from* the vector. For this example,\n",
        "        we find the closest match from a predefined list.\n",
        "\n",
        "        Args:\n",
        "            target_vector (np.ndarray): The vector to decode.\n",
        "            candidate_texts (list): A list of texts to compare against.\n",
        "\n",
        "        Returns:\n",
        "            str: The candidate text most similar to the target vector.\n",
        "        \"\"\"\n",
        "        candidate_vectors = self.encoder.encode(candidate_texts)\n",
        "\n",
        "        # Calculate cosine similarity between the target and all candidates\n",
        "        # The formula is: (A . B) / (||A|| * ||B||)\n",
        "        dot_products = np.dot(candidate_vectors, target_vector)\n",
        "        norm_target = np.linalg.norm(target_vector)\n",
        "        norm_candidates = np.linalg.norm(candidate_vectors, axis=1)\n",
        "\n",
        "        similarities = dot_products / (norm_target * norm_candidates)\n",
        "\n",
        "        # Find the index of the highest similarity score\n",
        "        best_match_index = np.argmax(similarities)\n",
        "\n",
        "        return candidate_texts[best_match_index]\n",
        "\n",
        "\n",
        "# --- Real-World Use Case: Generating a New Tech Product ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Initialize the ideator\n",
        "    ideator = LatentSpaceIdeator()\n",
        "\n",
        "    # 2. Define the 'Problem' and 'Variable' ideas we want to combine\n",
        "    problem_idea = \"A smart water bottle that tracks hydration and reminds you to drink.\"\n",
        "    variable_idea = \"A portable solar charger for electronic devices.\"\n",
        "\n",
        "    print(f\"\\nCombining Idea 1: '{problem_idea}'\")\n",
        "    print(f\"Combining Idea 2: '{variable_idea}'\")\n",
        "\n",
        "    # 3. Encode these ideas into the latent space\n",
        "    problem_vector = ideator.encode(problem_idea)\n",
        "    variable_vector = ideator.encode(variable_idea)\n",
        "\n",
        "    # 4. Explore the latent space by interpolating between the two ideas\n",
        "    # This creates a new vector representing a blend of the two concepts.\n",
        "    new_idea_vector = ideator.interpolate(problem_vector, variable_vector, weight=0.5)\n",
        "    print(\"\\nGenerated a new vector in the latent space by combining the two ideas.\")\n",
        "\n",
        "    # 5. Decode the new vector back into a human-readable idea.\n",
        "    # For this example, we'll use a predefined list of potential product ideas\n",
        "    # and find the one that is semantically closest to our new vector.\n",
        "    # A more advanced system would use a generative LLM for this step.\n",
        "    potential_solutions = [\n",
        "        \"A fitness tracker with a heart rate monitor.\",\n",
        "        \"A self-cleaning water bottle with UV-C light.\",\n",
        "        \"A water bottle that generates its own power from the sun to run smart features, like tracking intake and powering a purification system.\",\n",
        "        \"A backpack with an integrated solar panel.\",\n",
        "        \"A coffee mug that keeps your drink at the perfect temperature.\",\n",
        "        \"A smart bottle cap that fits any standard bottle and tracks sips.\",\n",
        "        \"A hydration pack for hiking that uses solar energy to cool the water.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSearching for the closest concept in our 'solution space'...\")\n",
        "\n",
        "    generated_idea = ideator.find_most_similar(new_idea_vector, potential_solutions)\n",
        "\n",
        "    # 6. Present the result\n",
        "    print(\"\\n--- New Product Idea ---\")\n",
        "    print(generated_idea)\n",
        "    print(\"------------------------\")\n",
        "    print(\"\\nThis idea was identified because its conceptual vector was closest to the 'average' of the smart water bottle and the solar charger vectors.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3N5w5hdNe8m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Latent Space Ideation with Advanced Swarm Queen Optimization (for Google Colab)\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS:\n",
        "# 1. Open a new notebook in Google Colab (https://colab.research.google.com/).\n",
        "# 2. In the menu, go to `Runtime` -> `Change runtime type` and select `T4 GPU` or another GPU.\n",
        "#    This is crucial as the decoder model is large and will be very slow on a CPU.\n",
        "# 3. Copy and paste the code from each section below into separate cells in your notebook.\n",
        "# 4. Run the cells in order.\n",
        "#\n",
        "# NOTE: Loading the models, especially the decoder, will take several\n",
        "# minutes and requires a significant amount of RAM.\n",
        "\n",
        "# ===========================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ===========================================\n",
        "# Uncomment and run this cell to install the required libraries in your Colab environment.\n",
        "# Using --upgrade ensures the latest library versions are installed.\n",
        "# !pip install --upgrade transformers torch sentence-transformers accelerate bitsandbytes einops\n",
        "\n",
        "# ===========================================\n",
        "# CELL 2: Imports and Setup\n",
        "# ===========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set up device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# CELL 3: Component 1 - Seed Encoder\n",
        "# ===========================================\n",
        "# This component encodes the initial text ideas into numerical vectors (embeddings).\n",
        "\n",
        "class TextEncoder:\n",
        "    \"\"\"Encodes a list of text strings into latent space vectors.\"\"\"\n",
        "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2', device=device):\n",
        "        print(f\"Loading encoder model: {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "        self.model.eval()\n",
        "        self.device = device\n",
        "        print(\"Encoder loaded.\")\n",
        "\n",
        "    def encode(self, texts: List[str]) -> torch.Tensor:\n",
        "        \"\"\"Encodes a list of texts into a tensor of embeddings.\"\"\"\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        return embeddings\n",
        "\n",
        "# ===========================================\n",
        "# CELL 4: Component 2 - Advanced Latent Space Explorer (Swarm Queen)\n",
        "# ===========================================\n",
        "# This is an enhanced version inspired by the user's scheduling/logic engine.\n",
        "# It includes a Queen that processes feedback and guides the swarm.\n",
        "\n",
        "class SwarmAgent:\n",
        "    \"\"\"Represents an agent in the swarm that generates a new idea vector.\"\"\"\n",
        "    def __init__(self, aid: int, alpha: float = 0.7, noise_scale: float = 0.05):\n",
        "        self.aid = aid\n",
        "        self.alpha = alpha\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def explore(self, queen_pool: torch.Tensor, peer_pool: torch.Tensor, queen_guidance: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generates a new vector by combining influence from a queen, a peer, and global guidance.\n",
        "        \"\"\"\n",
        "        queen = queen_pool[random.randint(0, len(queen_pool) - 1)]\n",
        "        peer = peer_pool[random.randint(0, len(peer_pool) - 1)]\n",
        "\n",
        "        # Move from peer towards the selected queen\n",
        "        primary_direction = self.alpha * queen + (1 - self.alpha) * peer\n",
        "\n",
        "        # Blend with the queen's global guidance vector\n",
        "        guided_direction = 0.8 * primary_direction + 0.2 * queen_guidance\n",
        "\n",
        "        # Add random noise for diversity\n",
        "        noise = self.noise_scale * torch.randn_like(guided_direction)\n",
        "        new_vector = guided_direction + noise\n",
        "        return new_vector\n",
        "\n",
        "class SwarmQueen:\n",
        "    \"\"\"\n",
        "    The Queen processes feedback (scores) and maintains a global model to guide the swarm.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_agents: int, vector_dim: int):\n",
        "        self.agent_weights = {i: 1.0 for i in range(num_agents)}\n",
        "        self.global_model_vector = torch.zeros(vector_dim, device=device) # Represents promising directions\n",
        "        self.previous_avg_score = 0.0\n",
        "\n",
        "    def process_feedback(self, agent_vectors: Dict[int, torch.Tensor], scores: List[float]):\n",
        "        \"\"\"\n",
        "        Updates agent weights and the global model based on the scores of the generated ideas.\n",
        "\n",
        "        Args:\n",
        "            agent_vectors: A dict mapping agent ID to the vector it produced.\n",
        "            scores: A list of scores corresponding to the ideas from agent_vectors.\n",
        "        \"\"\"\n",
        "        current_avg_score = sum(scores) / len(scores) if scores else 0\n",
        "        reward = current_avg_score - self.previous_avg_score\n",
        "\n",
        "        # Update agent weights based on reward\n",
        "        for aid in self.agent_weights:\n",
        "            if reward > 0:\n",
        "                self.agent_weights[aid] = min(1.5, self.agent_weights[aid] * (1 + reward * 0.1))\n",
        "            else:\n",
        "                self.agent_weights[aid] = max(0.5, self.agent_weights[aid] * (1 + reward * 0.1))\n",
        "\n",
        "        # Update the global model vector by moving towards high-scoring vectors\n",
        "        self.global_model_vector *= 0.9 # Decay the old model\n",
        "        for i, (aid, vector) in enumerate(agent_vectors.items()):\n",
        "            # High-scoring vectors have more influence\n",
        "            influence = scores[i] / 5.0 # Normalize score to 0-1 range\n",
        "            weighted_influence = influence * self.agent_weights[aid]\n",
        "            self.global_model_vector += 0.1 * weighted_influence * vector.to(device)\n",
        "\n",
        "        # Normalize the global model vector to prevent its magnitude from exploding\n",
        "        if torch.linalg.norm(self.global_model_vector) > 0:\n",
        "            self.global_model_vector /= torch.linalg.norm(self.global_model_vector)\n",
        "\n",
        "        self.previous_avg_score = current_avg_score\n",
        "\n",
        "    def get_guidance(self) -> torch.Tensor:\n",
        "        \"\"\"Returns the current global guidance vector.\"\"\"\n",
        "        return self.global_model_vector\n",
        "\n",
        "# ===========================================\n",
        "# CELL 5: Component 3 - Cross-Modal Projector\n",
        "# ===========================================\n",
        "# This neural network maps vectors from the encoder's space to the decoder's space.\n",
        "\n",
        "class Projector(nn.Module):\n",
        "    \"\"\"A simple MLP to project between embedding dimensions.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, (input_dim + output_dim) // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear((input_dim + output_dim) // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "# ===========================================\n",
        "# CELL 6: Component 4 - Decoder (Large Language Model)\n",
        "# ===========================================\n",
        "# This component generates human-readable text from a projected latent vector.\n",
        "\n",
        "class LatentToTextDecoder:\n",
        "    \"\"\"Generates text from a latent vector using a causal language model.\"\"\"\n",
        "    def __init__(self, model_name='google/gemma-2b-it', device=device):\n",
        "        print(f\"Loading decoder model: {model_name}... (This will take time and memory)\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map='auto',\n",
        "        )\n",
        "        self.model.eval()\n",
        "        print(\"Decoder loaded.\")\n",
        "\n",
        "    def generate(self, latent_vector: torch.Tensor, prompt_prefix: str = \"Generate a creative, detailed idea based on this concept: \", max_length=60) -> str:\n",
        "        \"\"\"Generates text by feeding the latent vector as a continuous prompt.\"\"\"\n",
        "        embedding_layer = self.model.get_input_embeddings()\n",
        "        prompt_tokens = self.tokenizer(prompt_prefix, return_tensors='pt').input_ids.to(latent_vector.device)\n",
        "        prompt_embeds = embedding_layer(prompt_tokens)\n",
        "\n",
        "        # Reshape the latent_vector to be 3D [batch, sequence, dim]\n",
        "        # before concatenating with the 3D prompt_embeds.\n",
        "        projected_embeds = latent_vector.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        inputs_embeds = torch.cat([prompt_embeds, projected_embeds], dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                max_new_tokens=max_length,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                temperature=0.9,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        prompt_length = prompt_tokens.shape[1]\n",
        "        generated_tokens = outputs[0][prompt_length:]\n",
        "        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "# ===========================================\n",
        "# CELL 7: Component 5 - Evaluator (LLM-as-a-Judge)\n",
        "# ===========================================\n",
        "# This function scores generated ideas. We use a dummy version for demonstration.\n",
        "\n",
        "def dummy_score_ideas(ideas: List[str]) -> List[float]:\n",
        "    \"\"\"Placeholder for an LLM-based evaluator. Assigns random scores.\"\"\"\n",
        "    print(f\"Scoring {len(ideas)} ideas (using dummy scores)...\")\n",
        "    return [random.uniform(3.0, 5.0) for _ in ideas]\n",
        "\n",
        "# ===========================================\n",
        "# CELL 8: The Full Pipeline (Updated with Advanced SQO)\n",
        "# ===========================================\n",
        "# This function orchestrates the entire process.\n",
        "\n",
        "def run_latent_sqo_pipeline(seed_ideas: List[str], iterations: int = 2, swarm_size: int = 20):\n",
        "    \"\"\"Runs the full ideation pipeline with the advanced Swarm Queen model.\"\"\"\n",
        "    # --- Initialization ---\n",
        "    encoder_dim = 384\n",
        "    decoder_dim = 2048\n",
        "\n",
        "    encoder = TextEncoder(device=device)\n",
        "    decoder = LatentToTextDecoder(device=device)\n",
        "    projector = Projector(input_dim=encoder_dim, output_dim=decoder_dim).to(device).half()\n",
        "\n",
        "    agents = [SwarmAgent(aid=i) for i in range(swarm_size)]\n",
        "    queen = SwarmQueen(num_agents=swarm_size, vector_dim=encoder_dim)\n",
        "\n",
        "    # --- Initial State ---\n",
        "    print(\"\\nEncoding initial seed ideas...\")\n",
        "    # *** FIX ***: Move the main vector storage to the CPU to avoid device mismatch errors.\n",
        "    all_vectors = encoder.encode(seed_ideas).cpu()\n",
        "    all_ideas = list(seed_ideas)\n",
        "    all_scores = dummy_score_ideas(all_ideas)\n",
        "\n",
        "    # --- Iterative Generation Loop ---\n",
        "    for i in range(iterations):\n",
        "        print(f\"\\n{'='*20} Iteration {i+1}/{iterations} {'='*20}\")\n",
        "\n",
        "        # 1. Identify Queens (top-k ideas)\n",
        "        top_k = min(5, len(all_ideas))\n",
        "        queen_indices = sorted(range(len(all_scores)), key=lambda k: all_scores[k], reverse=True)[:top_k]\n",
        "        queen_pool = all_vectors[queen_indices]\n",
        "\n",
        "        # 2. Explore: Each agent generates a new vector\n",
        "        print(\"Swarm is exploring latent space...\")\n",
        "        agent_vectors = {}\n",
        "        new_vectors_list = []\n",
        "        # *** FIX ***: Move the guidance vector to the CPU before passing to agents.\n",
        "        guidance_vector = queen.get_guidance().cpu()\n",
        "        for agent in agents:\n",
        "            new_vec = agent.explore(queen_pool, all_vectors, guidance_vector)\n",
        "            agent_vectors[agent.aid] = new_vec\n",
        "            new_vectors_list.append(new_vec)\n",
        "        new_vectors = torch.stack(new_vectors_list)\n",
        "\n",
        "        # 3. Project & Decode\n",
        "        print(\"Projecting and decoding new vectors into ideas...\")\n",
        "        projected_vectors = projector(new_vectors.to(device).half())\n",
        "        generated_ideas = [decoder.generate(vec) for vec in projected_vectors]\n",
        "        for j, idea in enumerate(generated_ideas):\n",
        "            print(f\"  -> New Idea {j+1}: {idea.strip()}\")\n",
        "\n",
        "        # 4. Evaluate new ideas\n",
        "        new_scores = dummy_score_ideas(generated_ideas)\n",
        "\n",
        "        # 5. Queen processes feedback\n",
        "        print(\"Queen is processing feedback...\")\n",
        "        queen.process_feedback(agent_vectors, new_scores)\n",
        "\n",
        "        # 6. Update State\n",
        "        all_ideas.extend(generated_ideas)\n",
        "        # *** FIX ***: Concatenate tensors on the CPU.\n",
        "        all_vectors = torch.cat([all_vectors, new_vectors], dim=0)\n",
        "        all_scores.extend(new_scores)\n",
        "        print(f\"Population size is now {len(all_ideas)} ideas.\")\n",
        "\n",
        "    # --- Final Result ---\n",
        "    sorted_ideas = [idea for _, idea in sorted(zip(all_scores, all_ideas), key=lambda pair: pair[0], reverse=True)]\n",
        "    return sorted_ideas\n",
        "\n",
        "# ===========================================\n",
        "# CELL 9: Example Usage\n",
        "# ===========================================\n",
        "# Define your initial seed ideas and run the pipeline.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_ideas = [\n",
        "        \"A drone that plants trees in deforested areas using biodegradable seed pods.\",\n",
        "        \"A smart coffee mug that uses kinetic energy from stirring to keep the drink warm.\",\n",
        "        \"Biodegradable shoes that contain seeds and can be planted at the end of their life.\",\n",
        "        \"An AI therapist chatbot that specializes in helping people with social anxiety.\",\n",
        "        \"A system for capturing and recycling greywater from showers for use in toilets.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Starting Latent Space Ideation Pipeline with Advanced SQO...\")\n",
        "    final_ideas = run_latent_sqo_pipeline(seed_ideas, iterations=2, swarm_size=10)\n",
        "\n",
        "    print(\"\\n\\n===================================\")\n",
        "    print(\"      Final Generated Ideas      \")\n",
        "    print(\"   (Sorted by dummy score)   \")\n",
        "    print(\"===================================\")\n",
        "    for i, idea in enumerate(final_ideas):\n",
        "        print(f\"{i+1}. {idea}\")"
      ],
      "metadata": {
        "id": "RPlRnoPRrzmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}